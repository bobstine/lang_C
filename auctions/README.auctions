$Id: README.auctions,v 1.2 2006/02/10 19:39:39 foster Exp $  -*- text -*-


TO DO:
------

Add some simple transformations: log(x), sgn(x) log(1+|x|), etc


BASIC SYSTEM OPERATION
----------------------

Here is an overview of how the system currently runs.

(1) The SV variables are created and put on the front of the list of
variables (see rkhs_svd/README for more details).  After this is done,
the system can't tell them apart from the regular X variables.  (We
can tell them apart by looking at their names--but theys stem doesn't
parse the names.)

(2) Then the variables are fed into the auction code (see
auction/auction.test.cc for the main routine of the auction.)

(3) The auction then finds the highest bidder out of the following
bunch:

        (a) There is an expert that takes these variables and bid
            that they are useful.
        (b) There is an interaction expert that tries products of
            variables that came from the stream.
        (c) There is an interaction expert that tries products of
            variables that have been added to the model.

As of version 3 of the code, these are all the experts that there
are.  I'm working on version 4 that adds one more class of expert--but
preliminary results suggest this expert is of theoretical interest
only.  It doesn't provide useful suggestions.

NOTE: The auction uses p-values for bids.  (Contrast this with using
information theory.)



DATA FILES:
-----------

There are two differnet ways of storing data--row major and column
major.  I haven't a clue as to which name goes with which concept.
Typically statistical files store one "row" of data per line.  (For
example, JMP and C4.5 both use this order.)  This is nice if you are
streaming over observations.  On the other hand, our auction code
requires one variable per line.  This means you can "stream" over
variables.  Hence we need to be able to convert from one format to the
other easilly.

The validation code is willing to run on much more data than can be
stored in memory at one time.  Hence it requires that data be stored
in the "one row per line" format.  (NOTE: currently it rolls all the
data into memory--but if this ability is needed, it is an easy patch.) 

The command:

	bob2rows

converts "variable streaming order" to "row streaming order".  This is
a very simple program with no options.  

The command:

	c45_parser

converts a C4.5 data set (which is in "row streaming order") to
variable streaming order.  It also wants a .names file.  It can do all
sorts of fancy things.  Try c45_parser --help for examples.

To combine two files that are in the usual "row streaming order" the
unix command "cat" works perfectly.  To combine two files that are in
variable streaming order requires a lot of manipulations.  Hence there
is a program called:

	bobcat a b c > d

that will concatinate a bunch of files and write out a combined file
on standard output.


OUTPUT FILES:
-------------

The auction code will generate several output files.  These are
traditionally called the following:  (e.g. they are given these names
both in auctions/* and in runs/*.)

	alpha.dat: Unknown function ? ? ? ? ? ?

	auction.model.txt: File for reading into traditional statistical
		package (say JMP)

	auction.model: actual model generated by the auction.  can be
		read by validator and by estimator.

	log: log of the run.  Includes useful information on
		successful variables and failed variables.

	costs: output of validator.  Has SSE and clasification errors
		for a variety of cost ratios.

	*summary: concatination of costs files.

PROGRAM FILES
-------------

DESCRIPTION OF MAJOR COMPONENTS OF THE AUCTION SYSTEM (by directory)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

runs/         
	A good place to see an overview of how to run the auction code

	contains sample data 

	README tells how to connect the data subdirectory

	c45_parser: converts C4.5 format to what Bob's code will read
		(same as seq_regr/c45_syntax.test.exec)
	auction:    runs an actual auction 
		(same as auctions/auction.test.exec)
	validator:  uses row order and a model to validate results
		(same as auctions/validator)
	estimator: Refits on a different data.  Efficiency hack.  Talk
		to Bob or Dean for justification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

auctions/  
        main auction code, incl. feature generation and selection
	
	Source code for:

		auction
		validator
		estimator

		
   run%: auction.test.exec validator estimator merge%.dat est%.rows 
val%.rows
  validator -f dataFile -n nameFile -m modelfile -s 
over_sampling_ratio_of_odds -o output
                reads model from modelfile, data from stdin/data;
                see validate_model.cc
   estimator  -f dataFile -n namesFile -m old_model_file -o 
new_model_file -c df
              refits a model to a new, larger data file; see 
estimate_model.cc
                       feature_factory - unary, interaction, and binary 
features
                       recommenders - select featuues for bidders
                       bidders - hold wealth and bid - alpha investing
                          see bidders.test.cc
                          currently only const and geometric bids defined!?

Makefile             Makes auctions/auctions.test.exec

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

seq_regr/ 

           Does one step of stepwise (or streaming).
           See seq_regr.test.cc for example use.
           Includes a command line interface: seq_regr_cmd.cc
           Should be possible to call from R, but we need to
                        compute the log-likelihood
                        and to remove variables that were added
                        (or recover previous state by storing the variable)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
mgs-sweep3/

	old sweep code

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Following are all support utilities

calibrate/
docs/
random/
ranges/
spline/
utils/

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Following are not used

LSCEmulator/
regression/
Alignment/
wavelets/
batch_regr/

