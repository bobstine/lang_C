Next... 

	Lag iterator/neighborhood type.

	More types of variables: differences, regional dummy vars, pop density

	Spatial context variables: exp smooth of past, national/state vars

	FeatureProductStream should only count down to the origin, starting
	with the squared term and then terminate.  Like others, should not
	square an indicator.  This is done?

	Need to handle output when one of the known, fixed experts is purged.
	Messes up the ordering the columns of the output file, as when experts
	arrive that were now known at the start of the auction.

Need to 

     Deans requests

        Track the "other" custom experts and show them in the csv summary.


     Memory leaks, exceptions

	Need way to throw an exception at time objects are initialized
	to signal all is not well (eg: blockSize in gsl_regression).


     General

        Put in a test to make sure number of variables does not come
        close to n. (occured in testing of Jason's program)

	What's up with the weird choices of polynomial streams.  Seems to raise
	variable to higher and higher power and never try another. Keeps trying
	to power up its prior choices.

	Interactions within a variable are a mess with things like State.

	Allow a variable to indicate the blocks rather than force
	equal block size as in current version.

	In parsing options, remove the [] option terms and use these to set attributes,
	as in type = indicator; label = xxx

	Build a tool to sample rows generated in modeling large files to make
	the JMP import faster.


     Streams 

	Interface code (such as current_feature) would be cleaner if the object
	providing used, skipped were made available to the streams that need it, and
	otherwise were hidden (perhaps add another template parameter for those that need it).

        Build smarter streams that look at the X's they are recommending, more careful
        check for which X's make more sense.  More checking before placing bids.
        At a minimum, use regular expression to avoid dummy variable products.
        EG: add a units, "domain" to the variables to suggest heuristic groups to
        attempt to use in PC analysis (ie, dont combine unless common domain).


     Experts

        Improve the auction history information (bid history, auction payoff history) by
	pairing this information or making it available in a more organized way to keep
	various lists synchronized.
        
        Build a fixed bidder that considers the terms left in the input stream, not
        just in the current order that is available.  Recycle through those not yet
        used, perhaps marking them with a boolean as cycles past to remove those that
        become constant (as in the old variable selection code used in BR model).


     Logistic regression

     	Get the cross-validation to run for least squares and logistic regression.
        Leave out subset works, but it is not plugged into any further.

        Need to implement protection levels in logistic regression.

        Figure out how to use calibration for logistic regression.
     

     Subspace bundles

     	Think about which things to combine, such as by using some sort of "units"
	attribute or within named feature sets.

	 *** Subspace methods are all giving pc's that are not well-scaled....


     Calibration

        Should more of this code be moved into the auction object, as done in the old
	version of the calibration adjustment? (For example, there's still the xb_feature
	function that builds a feature of the predicted values.)

     
     Printing, miscellaneous

        Fix printing on model summary for the intercept business, push more of the 'messy'
	things down into the stat_utils object rather than leaving in the gsl_regr object.
	Need a SE for the intercept at some point anyway?


     Numerics

        Check the gsl_engine code for "validation leakage" where it might use some
	of the validation data.  Found some of this in the gsl_engine::smooth routine.
	Need better, cleaner way to access the data held by the data object, such as
	return vector views of the "n" items in use for estimation rather than the 
	"length" items held in both training and test data.




------------------------------------------------------------------------------
28 May 10

       Revise input data format to use a compressed TAR archive.  Would like to be
       able to use fuse and avfs to mount the tar file as a directory, but that's
       not in os-x yet.

       Removed bug in handling of validation cases that somehow avoided doing the
       cross-validation (in build_model_data).


------------------------------------------------------------------------------
 1 May 10

        Convert input to use a "role" field in the description (second line)
	of a variable to identify its role in the model, with the choices being
	x, y, and context.  The in/out variable is a context variable.

	Fix the input to verify that it reads enough characters.

------------------------------------------------------------------------------
 5 Apr 10

       Verify that fit matches that from R.  Real pain due to lagging X's but
       not the response.

       Minor clean-ups, such as allowing interaction with several parents.


------------------------------------------------------------------------------
17 Mar 10

       Replace attribute map by a multi-map and then a map<string,set>
       to allow multiple values of a named attribute (eg, interacts_with).
       Big ripple since use set here and there to collect features, so need
       operator==, <.

       Attributes are sets, lag experts in place.  Many more custom experts
       created with added variables.  Have lag iterator/range.     

------------------------------------------------------------------------------
12 Mar 10

       Have the blocked white test in place, with 3000 counties so can try
       other blocking sizes (original number 3023 was prime!).

       Have the validation data working.

       Fix the progress.csv file so that aligns again; fixed error in calc
       of the initial out-of-sample CVSS.  Now done internally to the model
       rather than from the auction.test.cc file.
 

------------------------------------------------------------------------------
 7 Mar 10
 
	When a variable gets added to the model, where do you put the knowledge
	of what new streams (and experts) ought to join the auction?
  	  Seems that a feature would 'know' the most about what to add next (eg, a
	variable would know it is spatial, and so suggest adding spatial
	interactions).  But features don't know about streams.  Nor is it easy
	to see how to import the information about features at the time that the
	file is read.

	Approach is to interpret an attribute 'interact' of a feature as giving
	the attribute of other variables to combine it with.  This is done by
	having the auction have a list of all source features that can be scanned
	to identify 'raw' features with the indicated attribute.

	Clean up NaN problem that was happening in logistic regression when a 
	case would be assigned weight 0.

	Fix bug in constructing names of variables.  The arguments object was 
	used in the interaction object, but not all of the other features
	were filling this name in.  As a result, it "looked like" variables
	were entering the model more than once.  That was just a problem with
	the names: interactions only had the names of one of the parts.


------------------------------------------------------------------------------
 3 Mar 10
 
      Experts now have a role attribute and are more lightweight, with an
      envelope implementation like that of features.

      Parasitic experts that bid on rejected features now paid from bids,
      those that bid on used features created and paid from winnings on 
      initial bids.
      
      Introduce FeatureProductStreams to build interactions between features
      added to the model and other features in the model.  These are added
      to the collection of experts 'on the fly' as variables are added to the
      model.
    

------------------------------------------------------------------------------
 3 Jul 09    

       Small improvements to csv_parser; can now recognize [ ; ] options in 
       name of the variable.

       Fix f_test (stat_utils) so that it detects an underflow/overflow in 
       p-value calculations.


------------------------------------------------------------------------------
27 May 09      0.90

       Have recompiled with new feature structures in place.  Crashing, but it
       compiles.  Dies when offering an interaction feature.

       Now have running.  'Error' was the use of a copy constructor within
       a function, rather than to use a const& within the function. The feature
       created in interaction feature stream (feature_streams.Template.h)
       evidently vanished when the function ended.  It lost track of it, even
       though it was in the vector result.  Yuk.  

       Problem really was in the interaction feature itself.  It tried to hold its features
       as references rather than as actual values.  That was the error.

       New feature model (using -> to dereference object as explained in Coplein)
       works very nicely.  Handles memory well.  Valgrind reports no leaks


------------------------------------------------------------------------------
26 May 09 

       Get new style of reference counted features starting to work. Mimic style of
       the number class in Coplien.  Have the test program compiling and now need to
       stick it in the auction and see what happens.  A Feature is now a wrapper
       around the ABC and implementation, using operator -> to forward calls to the
       underlying range of data.

       Valgrind of my_features.test finds no leaks in this very basic test.


------------------------------------------------------------------------------
17 May 09   (v 0.80c)

	New type of expert (calibration expert) that offers bid only right after variable
	enters auction (watches how long since one goes into auction).  Expert has a
	'priority' that allows it to jump in line when it has a bid.  Bids in bursts that
	occasionally increase the alpha level after period of changes to model.

	Expert adds 2,3 powers of fit and if these are useful, then adds 4,5th
	powers. Tests show that it basically makes up for the wrong link function, but
	does so in a reasonable way.

	This needs: 
	     - expert must know when variable was just added to model
	     - new type of bidder that occasionally ups its bid (bursts).
	     - priority appears in sort for winning bidder
	     - expert/bidder needs to know how many rounds since variables added.
	     - constructs variables from Xb, fit of current model... model becomes stream
     	       but only when just having added a varible.


------------------------------------------------------------------------------
 8 May 09   (v 0.60)

   Calibration
        Spline calibration is out as a manual step, replaced by another expert. Would lose
        track of the basis if just add a single column, and not easy to extract certain
        basis without choosing knots.  Too much 'leakage' of y into X space.

   Pick off name of added variable rather than introduce added/decline column so 
   easier for Dean to get the variable name in his graphics.

   Add RSS and CVSS columns to progress.csv when adding a variable. Computed down
   in the model rather than exporting data what would require reassembling order.
   (new function is sums_of_squares).

   Spline calibration also present in this version, but is not incorporated in the
   fit; its only there as a check/diagnostic.


4 Apr 09

   Add level of protection option to allow various schemes for computing p-value.

   Add a second bidder on the file stream, using a universal bidder
   rather in addition to the fixed alpha bidder.  Build a bidder
   that's both Universal + Fixed alpha hybrid.  Several streams now
   have two bidders.

   Fixed bug that hangs with the universal bidder on the finite stream
   when reached the end but still tried to see what it had as a
   feature. Put in a test for has_feature before poping the feature stream.


------------------------------------------------------------------------------
23 Mar 09

   Improve output of the state of the auction by writing more information to
   progress.csv file in place of the old alpha file.

   Fixed negative sign bug in setting max bid that was causing expert
   to think it could not bid once its alpha exceeded 1.


------------------------------------------------------------------------------
22 Feb 09

   Improve the name placement in output to show the explanatory variables more 
   clearly in the tabular summary of the model.

   Debug fixed feature streams that were not fully spending all of their alpha.
   Bug was in bidders that stored the initial finite stream (bidders.h).

   Write some basic instructions on running the model executables.


------------------------------------------------------------------------------
22 Jul 08

   Auction now scans for singularity of the offered variables, and resets the
   auction to run again if this happens without billing for the variable.

------------------------------------------------------------------------------
13 Jul 08

   Trying to run program again.  Want to get back to have ability for auction to
   fit the BR data.

------------------------------------------------------------------------------
15 Feb 08

   Plug in the RKHS kernel method as a stream.  Kinda raw.  Should we standardize?  
   Which kernel do you use? ...

   Subspace methods are all giving pc's that are not well-scaled. These look singular,
   or the decomposition fails.  It should check to make sure that it has a subspace
   to offer before bidding.

------------------------------------------------------------------------------
29 Jan 08

   Have the GSL eigen decompositions working nicely, but need to
   figure out where the space for the eigen vectors that get created
   should be.  Who manages this space?
        
   Make something like a GSL feature?


------------------------------------------------------------------------------
28 Jan 08

   Start working on framework for adding principle component type stream.
	Bundle Streams
	Eigen tools in gsl library


------------------------------------------------------------------------------
27 Jan 08

Have polynomial experts in place and running.


------------------------------------------------------------------------------
21 Jan 08

Have auction running with new type of experts.


------------------------------------------------------------------------------
20 Jan 08

Quest to simplify things runs into a problem. On the feature side, can combine
heterogenous features since they have a common abstract base class whose functions
do *not* depend on the template classes.  FeatureABC is not a generic class.

On the expert side, the common functions *do* seem to depend on the generic 
classes. An expert is basically the cross-product of bidders with streams, so theres
little to offer for an ABC.  How do you put these all together?  You have to
isolate things that are generic, not requiring the class.  Put that in the ABC.

Build a basic ABC for the experts, and now can combine them.  Dean would call
these experts an "interface" class, made to put a common front on the underlying
bidders and streams.


------------------------------------------------------------------------------
17 Jan 08

Fiddle with bidders.  Do not show the obtained p-value, only the test outcome.


------------------------------------------------------------------------------
15 Jan 08

Leave out analysis now running for auction with linear model.

Numerous patches here and there to lower level code allows running auction. 
Fit a model to the First Funds data, using subset selection to reserve a test
set for subsequent cross-validation runs.

Get the fit, but not the reasonable R2 sort of summary stats.  t/p-value are
off compared to what JMP finds in test data. Odd that get the right estimates,
but not the right SE's and R2.  Turns out to have been that the output was
labeled wrong!


------------------------------------------------------------------------------
 4 Jan 08

Have the auction running now, using Richard's bank data as an interesting 
test.  Converted the auction to generic model class, and forgot about referencing
the auction.  Now that I fixed that, it's running well -- or so it seems.


------------------------------------------------------------------------------
20 Dec 07  

Screen out collinear predictors at evaluation.  Seems to run well now.  Not sure of
speed, so perhaps time for some shark feeding.


------------------------------------------------------------------------------
19 Dec 07  

Tests show running OK now, but that current method does not recognize when
an added variable is perfectly collinear (a copy, in fact) with the previously
used variables.  (Must be masked in some way by the weights?)

Test works well by recognizing the problem in running the auction, then using
emacs to find the interesting row, pasting into a file of interesting rows,
then use the rows-to-columns function (seqregr project) to transpose the data
and read into JMP.


------------------------------------------------------------------------------
18 Dec 07  

Running better, but not getting a decent fit.

Need to check that its not trying to square an indicator, and that the restore
prior status is working as it should.  Twice adds things (preliminary test) but
then dont' work so well in fit (complains about predictions at 0).


------------------------------------------------------------------------------
16 Dec 07    

Now compiling and running code. Not right, but running.

Need to sort out the nature of the recommenders, find out which things
they are building.  Logistic regression seems to run off the end and produce
zero weights for many observations, then dies.  Was reading wts as y, but
that should not cause it to diverge.

What the the apparently duplicate things being offered by the recommenders?
Resolved.

------------------------------------------------------------------------------
14 Dec 07    

Get to compile after much nashing of teeth.  Problems were from linking...
Got to remember to look at the build results when linker fails.  Operator<<
was over-loaded and confused the compiler...  


------------ Current

	Have to remove the calibrator in order to save the calibrator, unless
	the model 'remembers its fit' that was used prior to adding the calibrator.

	Building my_features.test.exec...
	Smoothing spline operator running into problems when try
	to build a feature with a smoothing spline.  Wants to copy
        construct, and that's not good.  But why so many times?

	Need for the spline to become an object with state so that it can
	become a feature and be stored.  Could write coefs to a file if needed.
	Need a calibrator object that acts like a feature.  Something like
	a unary range that could just compute f(y^) with y^ from another
	formula.

------------ Do next

The model ought to be responsible for reading/writing the calibration
function, with the auction, estimator, and validator having no idea
what sort of beast is being used for the calibration.  Leave all of
that encapsulated in the model, or in some template parameter that
becomes part of the model.

Add other types of recommenders that either pass through the data a
second time or use other methods to construct features.  At 800
rounds, pretty much done with the set of feature generators tried
here.

Model linear combination does not have to be recomputed as I am doing;
after all, the fit is known.

Speed up the caching of centers and scales; get more use from these,
particularly for the smoothing spline and other unary operators.

Get dummy property propagated better in ABC or cache for features.

Odds and ends

	Shorter, less jumbled names for linear combinations.

	Template more things in the feature factory so that not so
	bound to a vector of features.

	Feature factory ought to free space when done.


------------ Remember that

BIDDERS, RECOMMENDERS, and FEATURES are all abstract classes.

All get tossed around as pointers.  This is especially true for the
features.  Various things can modify a feature.

---

BIDDERS place all of the bids in the auction.  The auction only sees
the bidders, not the underlying recommenders or data columns.

BIDDERS come in a one to one correspondence with RECOMMENDERS.

RECOMMENDERS offer features to BIDDERS in the auction.  At the moment,
each bidder looks at only one recommender, but several bidders can bid
on the same recommender.

Recommenders come in two flavors.  One builds features directly from a
collection of columns.  The other uses some 'source' of features,
which can include the auction itself.

FEATURES are the items of the auction.  

COLUMNS are the repository of the data used to build the features, but
columns themselves are never offered directly by recommenders.


--------------  Dead Bugs

	Smoothing spline is not working correctly. I need to test smoothing 
	spline code with bigger problem to see if I can figure out why its dying
	when I build auction_test.

The value of the smoothing spline is not quite an average of the data,
so you can get values that are negative or larger than 1.  These
caused the logit function to return nan.  Logit now checks for a
proper range.

---

	Spline fit fails as well as SE jumps to huge value after get it
	to add dummy missing data indicator by switching to hybrid optimizer
	rather than pure newton method.

The spline code has a hard time finding lambda from the input df
unless the data are widely spread.  So, I just round to a 'coarse
grid' (1000 pts on [0,1]) and let the algorithm take care to join up
the associated y's.

---

	How should a model handle the calibrator column since it
	is not a feature like the other predictors.

Have to make a feature that is the smoothing spline or other
calibrator.  Otherwise cannot force the appropriate cross-validation
to happen.
