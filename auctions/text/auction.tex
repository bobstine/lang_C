\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{amssymb}
\usepackage{graphicx}

\input{/Users/bob/work/papers/standard}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

% --- Paragraph split
\setlength{\parskip}{0.00in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.4}

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\small Implementing the Auction}}}

\newcommand{\ras}[1]{\noindent{\textcolor{red}{\{{\bf ras:} \em #1\}}}}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}



% --- Title

\title{  
         Implementing the Auction
}

\author{
        Robert A. Stine                                      \\
        Department of Statistics                             \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          \\
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle 

\vspace{-0.5in} \centerline{\bf Abstract} 

\clearpage

\section{Tests in Least Squares} % -----------------------------------------------------------

 The value of the variable {\tt protection} (which is set in the constructor of
 an auction) determines how the code treats the various test statistics.  The
 higher the level of the protection, the more conservative the computation of
 the p-value associated with a test.  The current implementation uses the
 following methods, assumming that the current model includes $k$ predictors
 with an intercept and has residuals in the $n$-vector $e$.  Consider testing
 the effect of adding $k_{new}$ predictors that produce residuals $e_{new}$.

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the usual $F$ statistic for the added
 variable(s),
\begin{equation}
    F_0 = \frac{ dSS/k_{new} }{(e'e-dSS)/(n-1-k-k_{new})} \;.
\label{eq:f0}
\end{equation}
 Obtain the p-value by comparing $F_0$ to the $F$-distribution with $k_new$ and
 $n-1-k-k_{new}$ degrees of freedom.  Equation \eqn{eq:dss} gives the familiar
 expression for the change in the regression sum-of-squares obtained by adding
 the additional variables.

\item White estimator, standard $F$ test. In this case, the White estimator is
 used to compute the regression sum-of-squares, with an ``F-like'' statistic
\begin{equation}
    F_1 = \frac{ dSS_{white}/k_{new} }{(e'e-dSS_{white})/(n-1-k-k_{new})} \;.
\label{eq:f1}
\end{equation}
 Expression \eqn{eq:wdss} gives the White estimator used in the numerator of
 $F_1$.  The denominator of $F_1$ matches that of $F_0$.  As before, obtain the
 p-value by comparing $F_1$ to the $F$-distribution with the indicated degrees
 of freedom.

\item White estimator, conservative $F$ test. The White estimator is again used
 to find the regression SS, but now the denominator uses the residual sum of
 squares {\em without} adding the new variable,
\begin{equation}
    F_2 = \frac{ dSS_{white}/k_{new} }{e'e/(n-1-k-k_{new})} \;.
\label{eq:fstat}
\end{equation}
 Calcuate the p-value as for $F_1$.

\item Conservative p-value.  Not yet implemented.  The idea is to use a more
 robust procedure to find the p-value in place of the F-distribution.  I guess I
 was thinking of something like a Bennett p-value here applied to the White F
 stat.  That seems way too conservative at this point, and it probably makes
 more sense to adjust for possible dependence using a block calculation.

\end{enumerate}

\subsection{White variance estimates} % ---------     ----------     ---------

 Once the protection level in a least squares regression is larger than 0, we
 use a White (or sandwich) estimator to find the numerator of the $F$-statistic
 that is used to measure the statistical significance of an added variable(s).

 Consider the impact of adding new explanatory variables $X_{new}$ to a
 regression model that alread contains the variables $X$ and has residuals
 $e$. The first stage of testing sweeps out the prior explanatory variables $X$
 from the new variables, forming (Dean and Dong Yu omit this for speed.)
\begin{displaymath}
  Z = (I-X(X'X)^{-1}X') X_{new} = (I-H_X) X_{new}  \;.
\end{displaymath}
 Next we find the residual sum-of-squares of the augmented model.  Regress the
 current residuals $e$ on $Z$, obtaining the partial coefficients $c =
 (Z'Z)^{-1}Z'e$.  The increase in the regression sum-of-squares due to this
 partial regression is then the quadratic form
\begin{eqnarray}
  dSS &=& (Zc)'Zc = c'(Z'Z)c \cr
      &=& e'Z(Z'Z)^{-1}(Z'Z)(Z'Z)^{-1}Z'e \cr
      &=&  e'Z(Z'Z)^{-1}Z'e
\label{eq:dss}
\end{eqnarray}
 Note that $dSS$ is a $k\times k$ matrix if one adds $k$ variables. (The method
 {\tt change\_in\_rss} in gsl\_regr computes $dSS$ for the current regression.)


 To obtain the White version of this expression for $dSS$, begin by thinking a
 little differently about the matrix $(Z'Z)^{-1}$ in \eqn{eq:dss}. Using the
 usual OLS expressions for the partial regression, $\var(c) = s^2 (Z'Z)^{-1}$.
  Hence, we equate $(Z'Z)^{-1}$ with $\var(c)/s^2$. To get the White
 estimator that corresponds to $Z'Z$, we form the White estimate for $\var(c)$.
  The sandwich formula for the variance of the coefficients is
\begin{displaymath}
  \var(c) = (Z'Z)^{-1} Z'DZ (Z'Z)^{-1}, \quad D_{ij} = \delta_{ij} e_i^2\;.
\end{displaymath}
 Hence, the White estimator corresponding to $dSS$ in \eqn{eq:dss} replaces
$Z'Z$ with
\begin{equation}
 dSS_{white} = e'Z\left[\smfrac{1}{s^2}(Z'Z)^{-1} Z'DZ (Z'Z)^{-1}\right] Z'e  \;.
\label{eq:wdss}
\end{equation}
 This expression (and the resulting test) reduce to the usual sandwich test in
 the scalar case.  (The function {\tt white\_change\_in\_rss} passes the matrix
 shown in brackets in \eqn{eq:wdss} to the function {\tt change\_in\_rss}.)


\section{Tests in Logistic Regression} % --------------------------------------

 The current implementation resembles the protections offered in linear models,
 using the fact that a logistic regression is computed as a weighted least
 squares.  The distinction is with the White estimator: we use a Bennett bound
 for logistic regression.  The weights are used from the {\em prior} estimates
 rather than endogenously.  As in linear models, the variable {\tt protection}
 determines the nature of the testing procedure (really, the calculation of the
 p-value).  

 Given a set of explanatory varibles in the model $X$, the logistic regression
 solves the nonlinear estimating equations for $b$,
\begin{equation}
  X'y = X'p_b, \quad \mbox{ where } p_b = \frac{1}{1+\exp(-X'b)} \;.
\label{eq:esteq}
\end{equation}
 Note the similarity to the estimating equation (\aka, normal equation $X'y =
 X'(Xb)$) used in linear equations; in both cases, we choose $b$ so that the
 estimated values (either the probabilty $p$ or the fit $Xb$ reproduce the
 correlation of $Y$ with the explanatory variables.  (We subscript $p$ by the
 coefficient $b$ as a reminder of this dependence; later we simplify this to
 $p_1$ if the estimates are $b_1$. The same convention applies to other terms
 such as matrices constructed from the estimated probabilities.)

 Given a set of explanatory variables, we solve \eqn{eq:esteq} by using
 iteratively reweighted least squares.  The idea is a multivariate version of
 using a linear approximation to find the zero of a function.  Assume $f$ is an
 arbitrary function and we'd like to find a point $x^{*}$ for which $f(x^{*}) =
 0$.  Our current guess is $x_0$. Then a linear Taylor approximation gives us
\begin{displaymath}
  f(x) \approx f(x_0) + f'(x_0)(x - x_0)
\end{displaymath}
 implies that (since $f(x^{*}) = 0$ and we hope that the zero of $f$ is $\approx
 x^{*}$)
\begin{equation}
  x = x_0 + \frac{f(x)-f(x_0)}{f'(x_0)} = x_0-\frac{f(x_0)}{f'(x_0)} \;.
\label{eq:newton}
\end{equation}
 For logistic regression, $f$ is the derivative of the log of the likelihood.
  Let $p$ denote the estimated probabilities from the current fit; define the
 associated variance estimates as the diagonal of the matrix $V$,
\begin{displaymath}
  V_{ii} = p_i (1-p_i), \quad i=1,\ldots, n\;.
\end{displaymath}
 The estimating equation \eqn{eq:esteq} is the first derivative of the log of
 the likelihood function.  Deviations of $f(b) = X'(y-p_b)$ at the current
 solution mean that we need to change $b$ to improve the solution. The second
 derivative of the log-likelihood (which is the slope of the first derivative)
 is the matrix $X'VX$.  Hence, following the script of \eqn{eq:newton} we have
 the classic Newton iteration
\begin{equation}
  b_1 = b_0 - (X'V_0X)^{-1}X'(y-p_0) \;,
\label{eq:newton}
\end{equation}
 where $V_0$ is the diagonal matrix computed from $p_0 = 1/(1+e^{-x'b_0})$,
 which is the estimated probability at $b_0$.


 Rather than directly follow this Newton iteration, we convert the estimation to
 a regression problem so that we can use weighted least squares.  (This allows
 us to use the same code base for fitting linear and logistic regression
 models.)  Convergence of Newton's method produces a fixed point $b$ that
 satisfies
\begin{displaymath}
  b = b - (X'VX)^{-1}X'(y-p) \;.
\end{displaymath}
 (Note: this expression suggests a way to form the sandwich estimator.) In order
 to write this as a (weighted) least squares, note that in the usual expression
 for a GLS estimator, the variance matrix appears twice:
\begin{displaymath}
  \hat\beta = (X'W^{-1}X)^{-1}X'W^{-1}Y \;.
\end{displaymath}
 We need to squeeze another $V$ into the Newton step \eqn{eq:newton}.  We can
 write this as a weighted least squares by forming a so-called pseudo-response
\begin{equation}
 y^{*} = Xb + V^{-1}(y-p) \;.
\label{eq:ystar}
\end{equation}
  We then compute the next iteration as
\begin{equation}
  b_1 = (X'V_0X)^{-1}X'V_0y^{*} \;.
\label{eq:irls}
\end{equation}
 Notice that the variance matrix $V_0$ appears in the expression for $b_1$
 without an inverse that one might expect from the corresponding expression for
 the WLS estimator.  The explanation is the formation of the pseudo-response.
  If we treat $Xb$ on the r.h.s. of \eqn{eq:irls} as fixed, then heuristically
 $\var(y^{*}) = V^{-1}V V^{-1} = V^{-1}$ so $V_0$ {\em is} the inverser of the
 variance matrix of $y^{*}$.


 For testing, however, the weights are fixed at the values obtained by the
 current model only, without an update for the variable underconsideration.
  This is in keeping with the second level of protection used in the linear
 model.  We have found this sufficiently useful in logistic regression as to
 omit the more traditional updating.  (It also keeps the code much simpler.)
  The key function in the interface is {\tt add\_predictors\_if\_useful}.
 

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the Wald $F$ statistic which is the
 change in the log of the likelihood for the improvement in the model.
  We prefer the Wald statistic to the score test (basically, $t$
 statistics for the coefficient estimates) to avoid problems in which
 the slope is poorly determined but highly useful (\eg, a separating
 hyperplane -- or nearly so -- has been found).

\item Use Bennett's inequality.  {\bf the current version only works
 for one dim}.  The values $y_i$ of the response are assumed to be
 bounded within the range $m \le y_i \le M$.  If there's only one
 explanatory variable, the expression for the new slope introduced
 into the logistic regression by adding the new variable $x_new$
 becomes (simplifying the formula \ref{eq:irls})
\begin{displaymath}
  b_{new} = \frac{z'y}{z'z}
\end{displaymath}
 where $z$ and $y$ have been swept of the current explanatory variables in the
 weighted inner product define by the current variables.  That is, express the
WLS \eqn{eq:irls} estimate as the solution to an OLS with the weights
incorporated into the variables 
\begin{equation}
  b_1 = (X_v'X_v)^{-1}X_v'y^{*}_{v}, 
    \quad X_v = V_0^{1/2}X, y^{*}_v = V_0^{1/2}y^{*} \;.
\label{eq:wls}
\end{equation}
 Alternatively, return to the Newton iteration shown in \eqn{eq:newton}.  One
 need only test the size of the increment $(X'V_0X)^{-1}X'(y-p_0)$ in order to
 decide whether to consider adding the proposed variable.

\end{enumerate}

\subsection{Comments on the implementation}

 The same code base handles both linear and logistic regression.  At the top
 level in {\tt gsl\_model}, both linear and logistic regressions inherit from the
 {\tt gsl\_regression} object.  The distinction likes in the default choice of
 the ``engine'' that handles the details of the calculations.  Specifically,
 engines are distinguished by how they define the inner product for all
 calculations. (All regressions rely on the underlying model for inner products
 such as matrix multiplication, inversion, and solving systems of equations.)
  For a linear model, we use a standard engine that has the common dot-product
 $\ip{x}{y} = \sum_i x_i y_i = x'y$.  For a logistic regression, we use a
 weighted dot product $\ip{x}{y} = \sum_i x_i y_i w_i$.  The added complication
 is that the weights change in a logistic regression; hence, there are
 convenience functions that ``reweight'' the engine.  Engines in the code (see
 gsl\_engine.h) do not inherit from a common base (we never need a list of
 engines); rather they implement common {\em policies}.  

 Functions handled by engines include those that seem clear ({\tt average,
 sum\_of\_squares, blas\_ddot}) as well as the otherwise innocuous functions
\begin{itemize}
  \item {\tt prepare\_vector\_for\_analysis}
  \item {\tt insert\_analysis\_matrix}
\end{itemize}
 Preparing a vector is a simple copy (ols engine) or a copy plus scaling by the
 square root of the weights (for the WLS engine, as seen in the expression for
 the estimation shown in \eqn{eq:irls} above).  Inserting a matrix copies the
 matrix and scales the columns by the square root of the current weights
 $\sqrt{w_i}$.


 When a weighted engine is configured, the input includes a data object (one
 that supports the required data policy).  The weighted engine then extracts a
 {\em pointer} from the data object to the initial weights to use in
 calculations.  Changes to these weights affect subsequent calculations, but
 changes should be accompanied by telling the engine ({\tt
 weights\_have\_changed}).


 When a candidate feature is offered to a logistic model via {\tt
 add\_predictor\_if\_useful}, the following steps occur: (Multiple features are
 handled similarly.)
\begin{description}
 \item{Data preparation} ({\tt prepare\_predictors}) The proposed feature (call
 it $z$ is copied into local storage (from an iterator into a GSL matrix),
 centered (with its mean stored, a weighted mean if $W_0 \ne I$), and scaled by
 $\sqrt(W_0)$.
 \item{Sweep current model} Current explanatory variables are swept from the
 proposed new variable, producing ``residuals'' $z_{res}$.  This is done using
 the current weights $W_0$.
\end{description}

\end{document}
