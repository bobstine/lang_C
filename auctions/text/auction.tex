% -*- mode: LaTex; outline-regexp: "\\\\section\\|\\\\subsection";fill-column: 80; -*- 
\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{amssymb}
\usepackage{graphicx}

\input{/Users/bob/work/papers/standard}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

% --- Paragraph split
\setlength{\parskip}{0.00in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.4}

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\small Implementing the Auction}}}
\newcommand{\eps}{\epsilon}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}



% --- Title

\title{  
         Implementing the Auction
}

\author{
        Robert A. Stine                                      \\
        Department of Statistics                             \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          \\
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle 

\vspace{-0.5in} \centerline{\bf Abstract} 

\clearpage

\section{Experts} % --------------------------------------------------------------------------

 The auction uses several types of experts.  (In the code, these are identified
 by an enumerated type.)  Experts provide a way to search in a more 'depth
 first' rather than 'breadth first' manner.  With experts, one does not have to
 examine, for example, every linear effect before considering a second-order
 effect.
\begin{description}

\item[Source experts] produce new features for consideration; in general, source
 experts build variables from the columns of an input data array.  The features
 from a source expert could be generated from an algorithm or some other
 mechanism, but not from the progress of the auction itself.  Each problem
 generally has several source experts (as identified by the 'stream' attribute
 in the data file).  Splitting the data into several sources allows the auction
 to discover collections of variables of more interest rather than having to
 proceed through them all at once.  Source experts are assigned $\alpha$-wealth
 at the time that the auction is initialized.

\item[Parasitic experts] (or perhaps more politely called derived experts)
 construct features from those that have already made an appearance in the
 auction.  These might be, for instance, various spatial interactions of a
 feature that was initially rejected from the auction previously.  Some versions
 of the auction have used parasites that form interactions among variables in
 the model; these are now handled elsewhere.  The idea is that parasites ought
 to be the ``voice of the forgotten'' or rejected features.  Parasites obtain
 $\alpha$-wealth from a tax on bids.  The tax on bids is small since we expect a
 lot of bids relative to the number of accepted features.

\item[Custom experts] are spawned when a feature is added to the model.  By
 default, the addition of a feature to the model generates an expert that offers
 an interaction in that feature and any other features in the model.  The
 feature may itself add other experts as well.  For example, a feature might add
 time lags segmentation of itself (to allow spatial heterogeneity, for
 example), or perhaps specify interactions with a specific collection of
 features.  Custom features allow the original expert to focus on a simple task,
 say picking features from a large data source, rather than having the
 distraction of following the status of features that join the model.  Custom
 experts are funded from the pay-out received when a variable joins the model.

 As an example, one might have introduce a ``knot expert'' that offered possible
 places to segment the effect of a feature (think turbo smoothing).  This expert
 would be bound to a specific feature accepted into the model.  Because the
 expert is tied to one specific feature, it is funded from that feature.
  Suppose that the expert $\cal F$ recommends the feature $X$ to the auction,
 placing some bid amount $b$.  The variable $X$ joins the auction, earning
 pay-out $\omega$.  Some fraction of this pay-out goes directly back to the
 recommending expert $\cal F$, the rest of the pay-out goes to experts related to
 $X$.  By default, $X$ is bound to an expert that builds interactions of $X$
 with other features in the model.  Assume that $X$ also comes with a custom
 expert $\cal E$ that segments $X$ in the model, allowing nonlinear effects for
 $X$.  $\cal E$ unlike parasitic experts, gets created and funded from a tax on
 the pay-out on $X$ when $X$ is accepted.\footnote{The code that builds custom
 experts is in {\tt auction.Template.h}, specifically {\tt
 pay\_winning\_expert}.  Any feature added to the model automatically creates an
 expert that builds interactions among variables added to the model. In
 addition, a feature $X$ that has the attribute {\tt interact\_with} will also
 generate an interaction between $X$ and all features that have an attribute
 named by {\tt interact\_with}.  For example, if {\tt interact\_with} has value
 {\tt quarter}, then $X$ will be crossed with any feature that has the attribute
 {\tt quarter}.}

\item[Calibration experts] serve a very narrow role of calibrating the fit of
 the current model.  These are funded at initialization and operate by-and-large
 outside the usual auction process.  They are experts more in name alone to
 unify the coding.
\end{description}


 It is worthwhile to review the funding of experts.  The current algorithm funds
 experts using ``taxes'' in order to avoid adding further wealth beyond the
 initial wealth allocated at initialization.  The auction funds source during
 the initialization process, allocating the total initial $\alpha$-wealth given
 to the auction among them (typically equally).  (Calibration experts are also
 funded at initialization, but out of a separate pool of wealth.)  The auction
 funds parasitic experts during the bidding process.  Because parasitic experts
 bid on features that are rejected, these are funded by a ``tax'' on bids placed
 in the auction.  For example, suppose a source expert $\cal E$ bids $\alpha =
 0.01$ for the feature $X$. The auction taxes this bid at, say, 5\% and
 distributes the tax revenue $0.01 \times 0.05 = 0.0005$ among those experts
 that bid on rejected features.  The auction funds custom experts that bid on
 accepted features by taxing (at some possibly different -- and higher -- rate)
 the pay-out $\omega$ made to an expert when a feature is accepted into the
 model.



\section{Tests in Least Squares} % -----------------------------------------------------------

 The value of the variable {\tt protection} (which is set in the constructor of
 an auction) determines how the code treats the various test statistics.  The
 higher the level of the protection, the more conservative the computation of
 the p-value associated with a test.  The current implementation uses the
 following methods, assuming that the current model includes $k$ predictors
 with an intercept and has residuals in the $n$-vector $e$.  Consider testing
 the effect of adding $k_{new}$ predictors that produce residuals $e_{new}$.

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the usual $F$ statistic for the added
 variable(s),
\begin{equation}
    F_0 = \frac{ dSS/k_{new} }{(e'e-dSS)/(n-1-k-k_{new})} \;.
\label{eq:f0}
\end{equation}
 Obtain the p-value by comparing $F_0$ to the $F$-distribution with $k_new$ and
 $n-1-k-k_{new}$ degrees of freedom.  Equation \eqn{eq:dss} gives the familiar
 expression for the change in the regression sum-of-squares obtained by adding
 the additional variables.

\item White estimator, standard $F$ test. In this case, the White estimator is
 used to compute the regression sum-of-squares, with an ``F-like'' statistic
\begin{equation}
    F_1 = \frac{ dSS_{white}/k_{new} }{(e'e-dSS_{white})/(n-1-k-k_{new})} \;.
\label{eq:f1}
\end{equation}
 Expression \eqn{eq:wdss} gives the White estimator used in the numerator of
 $F_1$.  The denominator of $F_1$ matches that of $F_0$.  As before, obtain the
 p-value by comparing $F_1$ to the $F$-distribution with the indicated degrees
 of freedom.

\item White estimator, conservative $F$ test. The White estimator is again used
 to find the regression SS, but now the denominator uses the residual sum of
 squares {\em without} adding the new variable,
\begin{equation}
    F_2 = \frac{ dSS_{white}/k_{new} }{e'e/(n-1-k-k_{new})} \;.
\label{eq:fstat}
\end{equation}
 Calculate the p-value as for $F_1$.

\item Conservative p-value.  Not yet implemented.  The idea is to use a more
 robust procedure to find the p-value in place of the F-distribution.  I guess I
 was thinking of something like a Bennett p-value here applied to the White F
 stat.  That seems way too conservative at this point, and it probably makes
 more sense to adjust for possible dependence using a block calculation.

\end{enumerate}

\subsection{White variance estimates} % ---------     ----------     ---------

 Once the protection level in a least squares regression is larger than 0, we
 use a White (or sandwich) estimator to find the numerator of the $F$-statistic
 that is used to measure the statistical significance of an added variable(s).

 The White estimate is generally proposed as a method of estimating a
 heteroscedastic-robust standard error.  It is used in finding a more robust
 t-statistic.  Consider the impact of adding one or more new features $X_{new}$
 to a regression model that already contains the variables $X$ and has residuals
 $e$. The first stage of testing sweeps out the prior explanatory variables $X$
 from $X_{new}$, forming $Z$ defined as (Dean and Dong Yu omit this for speed.)
\begin{displaymath}
  Z = (I-X(X'X)^{-1}X') X_{new} = (I-H_X) X_{new}  \;.
\end{displaymath}


 As motivation, consider testing the value of adding only one new feature, which
 after sweeping out $X$ is the column vector $z$.  The slope for the added
 variable is $\hat\gamma = (z'e)/(z'z)$.  The White estimate of the variance of
 $\hat\gamma$ is
\begin{displaymath}
  \var_w(\hat\gamma) = \frac{\sum_i (z_ie_i)^2} {(z'z)^2} 
   = \frac{z'E^2z} {(z'z)^2} \;,
\end{displaymath}
 where $E$ is an $n \times n$ diagonal matrix with elements $E_{ii}=e_i$.  (Note
 that we use the residuals from the fit with $X$ alone, {\em not} both $X$ and
 $z$. See \citet{fosterstine04:bank} for an explanation of this choice.)  Hence, the
 square of the robust $t$-statistic is
\begin{equation}
  t^2 = \frac{\hat\gamma^2}{\var_w(\hat\gamma)} 
      = \frac{(z'e)^2}{z'E^2z} \;.
\label{eq:t2}
\end{equation}
 Alternatively, we can obtain the same test statistic using the approach found
 in building an $F$-statistic.  This will be useful since we will consider
 adding bundles of several features, and we will test these bundles as a group
 rather than one at a time.  Generally, an $F$-statistic in regression is the
 ratio the change in the regression mean-squares to the residual mean-squares.
  When adding $z$ to the model, the regression sum-of-squares increases by
 $\hat\gamma' z'z \hat\gamma$.  We need to change this expression to obtain an
 $F$-statistic that corresponds to the robust $t$-statistic.  In particular,
 think of $z'z$ in $\hat\gamma' (z'z) \hat\gamma$ as proportional to the inverse
 of the variance of $\hat\gamma$.  We then replace the usual least squares
 expression by that provided by the White estimator.  The resulting $F$-like
 statistic replaces
\begin{equation}
  \frac{\hat\gamma' (z'z) \hat\gamma}{(e'e)/(n-q)}=
  \hat\gamma\left(\frac{z'z}{(e'e)/(n-q)}\right) \hat\gamma 
\end{equation}
by
\begin{equation}
  \hat\gamma' (\var_w(\hat\gamma))^{-1} \hat\gamma = 
  \left(\frac{z'e}{z'z}\right) \left(\frac{z'E^2z}{(z'z)^2}\right)^{-1}
  \left(\frac{z'e}{z'z}\right) = \frac{(z'e)^2}{z'E^2z} \;,
\label{eq:F2}
\end{equation}
 which matches the squared robust $t$-statistic in \eqn{eq:t2}.


 In the case of several added variables, we need to form the corresponding
 matrices.  The calculations are more messy, but follow the same steps.  We
 start by finding the increase in the regression sum-of-squares.  Regress the
 current residuals $e$ on $Z$, obtaining the partial coefficients $\hat\gamma =
 (Z'Z)^{-1}Z'e$.  The increase in the regression sum-of-squares due to this
 partial regression is then the quadratic form
\begin{eqnarray}
   \hat\gamma'(Z'Z)\hat\gamma \;,
\label{eq:dss}
\end{eqnarray}
 and the $F$0-statistic normalizes this expression by an estimate of the error
 variation.\footnote{The method {\tt change\_in\_rss} in gsl\_regr computes
 $\Delta SS$ for the current regression.}  The associated $F$-statistic for
 adding $m$ new features to the model is
\begin{equation}
  F = \frac{\Delta SS}{m \; s^2} 
    = \frac{1}{m} \; \hat\gamma' \var(\hat\gamma)^{-1} \hat\gamma \;,
\label{eq:F}
\end{equation}
where, if we continue to conservatively estimate the scale, $s^2 = (e'e)/(n-q)$.


 To obtain the White version of the $F$-test, replace the variance estimate in
 \eqn{eq:F} with the corresponding White estimator.  The sandwich formula for
 the variance of the coefficients is
\begin{displaymath}
  \var_w(\hat\gamma) = (Z'Z)^{-1} Z'E^2Z (Z'Z)^{-1}, 
  \quad E_{ij} = \delta_{ij} e_i\;.
\end{displaymath}
 Hence, the White version of the $F$-statistic \eqn{eq:F} is
\begin{eqnarray}
  F_w
 &=& \smfrac{1}{m} \;
     e'Z(Z'Z)^{-1}\left[(Z'Z)^{-1}Z'E^2Z(Z'Z)^{-1}\right]^{-1}(Z'Z)^{-1}Z'e \cr
 &=& \smfrac{1}{m} \; e'Z (Z'E^2Z)^{-1} Z'e 
\label{eq:Fw}
\end{eqnarray}
 (Compare to the scalar version \eqn{eq:F2}.)\footnote{The function {\tt
 white\_change\_in\_rss} passes the matrix shown in brackets in \eqn{eq:wdss} to
 the function {\tt change\_in\_rss}.}

\subsection{White with Blocks}

 Suppose that we treat the observations as possibly dependent in blocks.  That
 is, assume that the data $Y' = (Y_1', Y_2', \ldots, Y_K')$ come in $K$ clusters
 with covariance matrix $V_k$ within the $k$th group:
\begin{displaymath}
  Y = \mu + \epsilon, \quad \Var(\epsilon) = 
     \left(
     \begin{array}{ccccc}
       V_1     & 0      & \cdots & 0      \cr
        0      & V_2    & \cdots & 0      \cr
       \vdots  & \vdots & \ddots & \vdots \cr
        0      & 0      & \cdots & V_K   
     \end{array}  
     \right)
\end{displaymath}
 The usual White estimator is robust to heteroscedasticity, but not
 dependence. In this case, such as longitudinal data, we form a blocked White
 estimator. 

 For convenience, suppose that each of the $K$ groups is of size $b$ so that $n
 = K b$.  The blocked White estimate of the variance of the least squares
estimates $\hat\beta$ based on a collection of $q$ features $X$ is
\begin{equation}
  \var(\hat\beta) = (X'X)^{-1} \left(\sum_k X_k'e_ke_k'X_k\right) (X'X)^{-1},
\label{eq:blockwhite}
\end{equation}
 where $e_k$ are the residuals within the $k$th block.  To express these more
 directly, let $B_k$ denote an $b \times n$ matrix with an $b\times b$ identity
 matrix $I_b$ positioned to extract rows associated with block $k$:
\begin{equation}
  B_k = \left(
   \begin{array}{ccccccc}
     0  & \cdots & 0 & \underbrace{I_b}_{\mbox{\scriptsize block $k$}} & 0 & \cdots & 0 
   \end{array}
   \right) \;.
\label{eq:Bk}
\end{equation}
 If $e = Y - X \hat\beta$ denote the residuals from the current fit, then $B_ke
 = e_k$, the residuals in block $k$.  The expression for the blocked variance
estimate \eqn{eq:blockwhite} then becomes
\begin{eqnarray}
  \var(\hat\beta) 
  &=& (X'X)^{-1} \left(\sum_k X_k' B_k e \, e' B_k' X_k\right) (X'X)^{-1} \cr
  &=& (X'X)^{-1} \left(\sum_k X_k' B_k (I-H)YY'(I-H) B_k' X_k\right)(X'X)^{-1}.
\label{eq:blockwhite1}
\end{eqnarray}
 If $\ev Y = \mu$ lies in the column span of the current features $X$, then $H
 \mu = \mu$ so that $(I - H)Y = \epsilon$, and the variance expression becomes
\begin{eqnarray}
  \var(\hat\beta) 
  &=& (X'X)^{-1} \left(\sum_k X_k' B_k \eps \eps' B_k' X_k\right)(X'X)^{-1}\cr
  &=& (X'X)^{-1} \left(\sum_k X_k' \eps_k \eps_k' X_k\right)(X'X)^{-1}
\label{eq:blockwhite2}
\end{eqnarray}
 Taking expected values, the estimator is unbiased for the variance of the
 estimator (ignoring issues of selection, of course)
\begin{equation}
  \ev \var(\hat\beta) 
  = (X'X)^{-1} \left(\sum_k X_k' V_k X_k\right)(X'X)^{-1}.
\label{eq:blockwhite3}
\end{equation}

 To gain some intuition about the blocked White estimator defined in
 \eqn{eq:blockwhite}, it is helpful to compare it to a direct estimator of the
 variance of the slope.  Let $\hat\beta_k$ denote the estimated slope computed
 within the $k$th block.  It is then possible to show that blocked White
 variance approximates the observed variance of $\hat\beta_k$ under some
 conditions \citep{fosterlin10}.  For convenience, assume that the features $X$
 are identical within each block,
\begin{displaymath}
   X = 
    \left( \begin{array}{c}
       X_1 \cr  X_2 \cr \vdots \cr X_K 
    \end{array} \right)
    = 
    \left( \begin{array}{c}
       X_1 \cr  X_1 \cr \vdots \cr X_1 
    \end{array} \right)
\end{displaymath}
 With this conformity, $\hat\beta_k = (X_1'X_1)^{-1}X_1'Y_k$, and the
 overall slope fit to all of the data is the average of the $\hat\beta_k$:
\begin{displaymath}
  \hat\beta = (X'X)^{-1}X'Y 
       = \frac{1}{K} \underbrace{(X_1'X_1)^{-1} \sum_k X_1'Y_k}_{\hat\beta_k}
       = (X_1'X_1)^{-1} X_1' \ol{Y} \;,
\end{displaymath}
 where the $b \times 1$ vector $\ol{Y}$ is the mean response under the repeated
 conditions. Since the first row, for example, of each block is the same, the
 estimator simply averages the first $y$'s and uses the average in the
 regression.

 This homogeneity shows up in the blocked White estimator as well. The variance
 estimate \eqn{eq:blockwhite} reduces to
\begin{eqnarray}
  \var(\hat\beta) 
   &=& \frac{1}{K^2}
       (X_1'X_1)^{-1} \left(\sum_k X_1'e_k e_k'X_1\right) (X_1'X_1)^{-1} \cr
   &=& \frac{1}{K^2}\sum_k (X_1'X_1)^{-1}X_1'(Y_k-X_1\hat\beta)
                           (Y_k-X_1\hat\beta)'X_1 (X_1'X_1)^{-1} \cr
   &=& \frac{1}{K^2}\sum_k (\hat\beta_k-\hat\beta)(\hat\beta_k-\hat\beta)'\cr
   &=& \frac{1}{K^2}\sum_k (\hat\beta_k-\ol{\hat\beta_k})
                           (\hat\beta_k-\ol{\hat\beta_k})'  \cr
   &=& \frac{K-1}{K^2} \var(\hat\beta_k) \;,
\end{eqnarray}
 as in $\var(\ol{x}) = s_x^2/n$.  The difference between the direct estimator
 and the blocked White estimator lies in the information matrix $X_k'X_k$ for
 the blocks.  The larger the differences among the blocks, the greater the
 difference between the direct and White estimates. \citet{fosterlin10} discuss
 these differences further.

\section{Tests in Logistic Regression} % -----------------------------------------------------

 The current implementation resembles the protections offered in linear models,
 using the fact that a logistic regression is computed as a weighted least
 squares.  The distinction is with the White estimator: we use a Bennett bound
 for logistic regression.  The weights are used from the {\em prior} estimates
 rather than endogenously.  As in linear models, the variable {\tt protection}
 determines the nature of the testing procedure (really, the calculation of the
 p-value).  

 Given a set of explanatory variables in the model $X$, the logistic regression
 solves the nonlinear estimating equations for $b$,
\begin{equation}
  X'y = X'p_b, \quad \mbox{ where } p_b = \frac{1}{1+\exp(-X'b)} \;.
\label{eq:esteq}
\end{equation}
 Note the similarity to the estimating equation (\aka, normal equation $X'y =
 X'(Xb)$) used in linear equations; in both cases, we choose $b$ so that the
 estimated values (either the probability $p$ or the fit $Xb$ reproduce the
 correlation of $Y$ with the explanatory variables.  (We subscript $p$ by the
 coefficient $b$ as a reminder of this dependence; later we simplify this to
 $p_1$ if the estimates are $b_1$. The same convention applies to other terms
 such as matrices constructed from the estimated probabilities.)

 Given a set of explanatory variables, we solve \eqn{eq:esteq} by using
 iteratively reweighted least squares.  The idea is a multivariate version of
 using a linear approximation to find the zero of a function.  Assume $f$ is an
 arbitrary function and we'd like to find a point $x^{*}$ for which $f(x^{*}) =
 0$.  Our current guess is $x_0$. Then a linear Taylor approximation gives us
\begin{displaymath}
  f(x) \approx f(x_0) + f'(x_0)(x - x_0)
\end{displaymath}
 implies that (since $f(x^{*}) = 0$ and we hope that the zero of $f$ is $\approx
 x^{*}$)
\begin{equation}
  x = x_0 + \frac{f(x)-f(x_0)}{f'(x_0)} = x_0-\frac{f(x_0)}{f'(x_0)} \;.
\label{eq:newton}
\end{equation}
 For logistic regression, $f$ is the derivative of the log of the likelihood.
  Let $p$ denote the estimated probabilities from the current fit; define the
 associated variance estimates as the diagonal of the matrix $V$,
\begin{displaymath}
  V_{ii} = p_i (1-p_i), \quad i=1,\ldots, n\;.
\end{displaymath}
 The estimating equation \eqn{eq:esteq} is the first derivative of the log of
 the likelihood function.  Deviations of $f(b) = X'(y-p_b)$ at the current
 solution mean that we need to change $b$ to improve the solution. The second
 derivative of the log-likelihood (which is the slope of the first derivative)
 is the matrix $X'VX$.  Hence, following the script of \eqn{eq:newton} we have
 the classic Newton iteration
\begin{equation}
  b_1 = b_0 - (X'V_0X)^{-1}X'(y-p_0) \;,
\label{eq:newton}
\end{equation}
 where $V_0$ is the diagonal matrix computed from $p_0 = 1/(1+e^{-x'b_0})$,
 which is the estimated probability at $b_0$.


 Rather than directly follow this Newton iteration, we convert the estimation to
 a regression problem so that we can use weighted least squares.  (This allows
 us to use the same code base for fitting linear and logistic regression
 models.)  Convergence of Newton's method produces a fixed point $b$ that
 satisfies
\begin{displaymath}
  b = b - (X'VX)^{-1}X'(y-p) \;.
\end{displaymath}
 (Note: this expression suggests a way to form the sandwich estimator.) In order
 to write this as a (weighted) least squares, note that in the usual expression
 for a GLS estimator, the variance matrix appears twice:
\begin{displaymath}
  \hat\beta = (X'W^{-1}X)^{-1}X'W^{-1}Y \;.
\end{displaymath}
 We need to squeeze another $V$ into the Newton step \eqn{eq:newton}.  We can
 write this as a weighted least squares by forming a so-called pseudo-response
\begin{equation}
 y^{*} = Xb + V^{-1}(y-p) \;.
\label{eq:ystar}
\end{equation}
  We then compute the next iteration as
\begin{equation}
  b_1 = (X'V_0X)^{-1}X'V_0y^{*} \;.
\label{eq:irls}
\end{equation}
 Notice that the variance matrix $V_0$ appears in the expression for $b_1$
 without an inverse that one might expect from the corresponding expression for
 the WLS estimator.  The explanation is the formation of the pseudo-response.
  If we treat $Xb$ on the r.h.s. of \eqn{eq:irls} as fixed, then heuristically
 $\var(y^{*}) = V^{-1}V V^{-1} = V^{-1}$ so $V_0$ {\em is} the inverser of the
 variance matrix of $y^{*}$.


 For testing, however, the weights are fixed at the values obtained by the
 current model only, without an update for the variable under consideration.
  This is in keeping with the second level of protection used in the linear
 model.  We have found this sufficiently useful in logistic regression as to
 omit the more traditional updating.  (It also keeps the code much simpler.)
  The key function in the interface is {\tt add\_predictors\_if\_useful}.
 

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the Wald $F$ statistic which is the
 change in the log of the likelihood for the improvement in the model.
  We prefer the Wald statistic to the score test (basically, $t$
 statistics for the coefficient estimates) to avoid problems in which
 the slope is poorly determined but highly useful (\eg, a separating
 hyperplane -- or nearly so -- has been found).

\item Use Bennett's inequality.  {\bf the current version only works
 for one dim}.  The values $y_i$ of the response are assumed to be
 bounded within the range $m \le y_i \le M$.  If there's only one
 explanatory variable, the expression for the new slope introduced
 into the logistic regression by adding the new variable $x_new$
 becomes (simplifying the formula \ref{eq:irls})
\begin{displaymath}
  b_{new} = \frac{z'y}{z'z}
\end{displaymath}
 where $z$ and $y$ have been swept of the current explanatory variables in the
 weighted inner product define by the current variables.  That is, express the
WLS \eqn{eq:irls} estimate as the solution to an OLS with the weights
incorporated into the variables 
\begin{equation}
  b_1 = (X_v'X_v)^{-1}X_v'y^{*}_{v}, 
    \quad X_v = V_0^{1/2}X, y^{*}_v = V_0^{1/2}y^{*} \;.
\label{eq:wls}
\end{equation}
 Alternatively, return to the Newton iteration shown in \eqn{eq:newton}.  One
 need only test the size of the increment $(X'V_0X)^{-1}X'(y-p_0)$ in order to
 decide whether to consider adding the proposed variable.

\end{enumerate}

\subsection{Comments on the implementation}

 The same code base handles both linear and logistic regression.  At the top
 level in {\tt gsl\_model}, both linear and logistic regressions inherit from the
 {\tt gsl\_regression} object.  The distinction likes in the default choice of
 the ``engine'' that handles the details of the calculations.  Specifically,
 engines are distinguished by how they define the inner product for all
 calculations. (All regressions rely on the underlying model for inner products
 such as matrix multiplication, inversion, and solving systems of equations.)
  For a linear model, we use a standard engine that has the common dot-product
 $\ip{x}{y} = \sum_i x_i y_i = x'y$.  For a logistic regression, we use a
 weighted dot product $\ip{x}{y} = \sum_i x_i y_i w_i$.  The added complication
 is that the weights change in a logistic regression; hence, there are
 convenience functions that ``reweight'' the engine.  Engines in the code (see
 gsl\_engine.h) do not inherit from a common base (we never need a list of
 engines); rather they implement common {\em policies}.  

 Functions handled by engines include those that seem clear ({\tt average,
 sum\_of\_squares, blas\_ddot}) as well as the otherwise innocuous functions
\begin{itemize}
  \item {\tt prepare\_vector\_for\_analysis}
  \item {\tt insert\_analysis\_matrix}
\end{itemize}
 Preparing a vector is a simple copy (OLS engine) or a copy plus scaling by the
 square root of the weights (for the WLS engine, as seen in the expression for
 the estimation shown in \eqn{eq:irls} above).  Inserting a matrix copies the
 matrix and scales the columns by the square root of the current weights
 $\sqrt{w_i}$.


 When a weighted engine is configured, the input includes a data object (one
 that supports the required data policy).  The weighted engine then extracts a
 {\em pointer} from the data object to the initial weights to use in
 calculations.  Changes to these weights affect subsequent calculations, but
 changes should be accompanied by telling the engine ({\tt
 weights\_have\_changed}).


 When a candidate feature is offered to a logistic model via {\tt
 add\_predictor\_if\_useful}, the following steps occur: (Multiple features are
 handled similarly.)
\begin{description}
 \item{Data preparation} ({\tt prepare\_predictors}) The proposed feature (call
 it $z$ is copied into local storage (from an iterator into a GSL matrix),
 centered (with its mean stored, a weighted mean if $W_0 \ne I$), and scaled by
 $\sqrt(W_0)$.
 \item{Sweep current model} Current explanatory variables are swept from the
 proposed new variable, producing ``residuals'' $z_{res}$.  This is done using
 the current weights $W_0$.
\end{description}

\subsection{Reading and handling data}

 At the level of the auction, only numerical data is processed.  The auction
 code does not handle missing data nor indicators.  Any indication of
 missingness or role in the analysis must be handled prior to the auction, such
 as by the {\tt csv\_parser} function that uses the boost regex functions.

 Numerical data is read from a file aud converted into two vectors of {\tt
 Column} objects.  The read step is done by the routine {\tt
 insert\_columns\_from\_file} called from the main function that sets up the
 auction.  A {\tt Column} is a wrapper (envelope) class for the column data
 object that holds the actual data; this avoids the overhead of copying.  Column
 objects are lightweight.  The {\tt ColumnData} object just reads numbers
 (doubles, in particular), then computes summary characteristics (including the
 number of unique values and means) from the input values in {\tt
 init\_properties}.  It knows nothing about the interpretation of names or
 properties.  As far as a {\tt Column} is concerned, names and descriptions are
 simply strings.  (The only conversion that happens in columns is to fill spaces
 in names with the underscore character, converting for example ``the var'' into
 ``the\_var''.)

 The auction code carefully examines the names of the leading input variables to
 determine how to use the supplied cases.  Cases may be of three types:
\begin{itemize}
\item Missing cases.  These supply information about possible explanatory
 variables, but the not the response.  These are not predicted and are only used
 for information about features (such as lagged variable values).  These do not
 appear in the saved data file produced at the end of an analysis and do not
 appear in the data object ({\tt gsl\_data}) that supports the regression model.
\item In-sample or estimation cases.  These are complete $(y_i,\xx_i)$ tuples
 used to identify the model.
\item Out-of-sample or validation cases. These are complete cases, but will not
 be used in the fitting process.  They only appear for calculation of
 predictions for validation purposes.
\end{itemize}
 The input command-line argument ``x'' denote the number of leading cases which
 are formally missing the response.  These {\em must} be the leading cases in
 the data file.  These cases are neither used in fitting nor validation; they
 are simply there for building predictive features.  These rows are skipped over
 when transferring the response ({\tt build\_model\_data}) and accepted features
 into the auction.  (Each expert knows rows to skip in the model.)  If the
 leading column is named ``[in/out][in]'', then the first data column is treated
 as a boolean indicator of the in-sample and out-of-sample cases.  (For extra
 cases, the values of any response column -- indicator and numerical value --
 are ignored.)


 The parsing of the names of variables and the descriptions (property lists)
 happens when the columns are convered into features.  Features are initialized
 from columns.  As with columns, a feature is a wrapper hiding the particular
 type of variable.  {\tt ColumnFeatures} hold a column; other types, such as
 unary features, hold a function that converts the values in a another feature.
  The function {\tt add\_attributes\_from\_paired\_list}, for example, is called by
 a column feature to convert the ``description string'' read by a column into
 the paired property list used by a feature.  Columns, for example, might simply
 use this string as purely documenation, as in free text.  Features, in
 contrast, expect to find that this string has a paired list of terms.

 Columns and Features live separately from the code that actually fits the
 regression. The GSL routines, such as those in {\tt gsl\_regr}, work with a
 distinct data object, essentially a plain old C array.  The reason for this
 duplication is to allow outside routines, such as features, to have a mix of
 validation and estimation data, whereas the data inside the regression model is
 {\em only} that which is used for fitting the model.  Also, this separation
 makes it easy to handle the use of split samples for validation. Code for {\tt
 for} loops needs to be able to iterate over the elements of the data very
 quickly, so we pack the estimation cases adjacent to each other by permuting
 the input data.  The validation cases are sorted away from the estimation cases
 in the GSL objects, whereas they might be mixed together in the features.


 \bibliography{/Users/bob/work/references/stat}
 \bibliographystyle{/Users/bob/work/papers/bst/rss}

\end{document}
