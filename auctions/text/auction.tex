\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{amssymb}
\usepackage{graphicx}

\input{/Users/bob/work/papers/standard}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

% --- Paragraph split
\setlength{\parskip}{0.00in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.4}

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\small Implementing the Auction}}}

\newcommand{\ras}[1]{\noindent{\textcolor{red}{\{{\bf ras:} \em #1\}}}}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}



% --- Title

\title{  
         Implementing the Auction
}

\author{
        Robert A. Stine                                      \\
        Department of Statistics                             \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          \\
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle 

\vspace{-0.5in} \centerline{\bf Abstract} 

\clearpage

\section{Tests in Least Squares} % -----------------------------------------------------------

 The value of the variable {\tt protection} (which is set in the constructor of
 an auction) determines how the code treats the various test statistics.  The
 higher the level of the protection, the more conservative the computation of
 the p-value associated with a test.  The current implementation uses the
 following methods, assumming that the current model includes $k$ predictors
 with an intercept and has residuals in the $n$-vector $e$.  Consider testing
 the effect of adding $k_{new}$ predictors that produce residuals $e_{new}$.

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the usual $F$ statistic for the added
 variable(s),
\begin{equation}
    F_0 = \frac{ dSS/k_{new} }{(e'e-dSS)/(n-1-k-k_{new})} \;.
\label{eq:f0}
\end{equation}
 Obtain the p-value by comparing $F_0$ to the $F$-distribution with $k_new$ and
 $n-1-k-k_{new}$ degrees of freedom.  Equation \eqn{eq:dss} gives the familiar
 expression for the change in the regression sum-of-squares obtained by adding
 the additional variables.

\item White estimator, standard $F$ test. In this case, the White estimator is
 used to compute the regression sum-of-squares, with an ``F-like'' statistic
\begin{equation}
    F_1 = \frac{ dSS_{white}/k_{new} }{(e'e-dSS_{white})/(n-1-k-k_{new})} \;.
\label{eq:f1}
\end{equation}
 Expression \eqn{eq:wdss} gives the White estimator used in the numerator of
 $F_1$.  The denominator of $F_1$ matches that of $F_0$.  As before, obtain the
 p-value by comparing $F_1$ to the $F$-distribution with the indicated degrees
 of freedom.

\item White estimator, conservative $F$ test. The White estimator is again used
 to find the regression SS, but now the denominator uses the residual sum of
 squares {\em without} adding the new variable,
\begin{equation}
    F_2 = \frac{ dSS_{white}/k_{new} }{e'e/(n-1-k-k_{new})} \;.
\label{eq:fstat}
\end{equation}
 Calcuate the p-value as for $F_1$.

\item Conservative p-value.  Not yet implemented.  The idea is to use a more
 robust procedure to find the p-value in place of the F-distribution.  I guess I
 was thinking of something like a Bennett p-value here applied to the White F
 stat.  That seems way too conservative at this point, and it probably makes
 more sense to adjust for possible dependence using a block calculation.

\end{enumerate}

\subsection{White variance estimates} % ---------     ----------     ---------

 Once the protection level in a least squares regression is larger than 0, we
 use a White (or sandwich) estimator to find the numerator of the $F$-statistic
 that is used to measure the statistical significance of an added variable(s).

 Consider the impact of adding new explanatory variables $X_{new}$ to a
 regression model that alread contains the variables $X$ and has residuals
 $e$. The first stage of testing sweeps out the prior explanatory variables $X$
 from the new variables, forming (Dean and Dong Yu omit this for speed.)
\begin{displaymath}
  Z = (I-X(X'X)^{-1}X') X_{new} = (I-H_X) X_{new}  \;.
\end{displaymath}
 Next we find the residual sum-of-squares of the augmented model.  Regress the
 current residuals $e$ on $Z$, obtaining the partial coefficients $c =
 (Z'Z)^{-1}Z'e$.  The increase in the regression sum-of-squares due to this
 partial regression is then the quadratic form
\begin{eqnarray}
  dSS &=& (Zc)'Zc = c'(Z'Z)c \cr
      &=& e'Z(Z'Z)^{-1}(Z'Z)(Z'Z)^{-1}Z'e \cr
      &=&  e'Z(Z'Z)^{-1}Z'e
\label{eq:dss}
\end{eqnarray}
 Note that $dSS$ is a $k\times k$ matrix if one adds $k$ variables. (The method
 {\tt change\_in\_rss} in gsl\_regr computes $dSS$ for the current regression.)


 To obtain the White version of this expression for $dSS$, begin by thinking a
 little differently about the matrix $(Z'Z)^{-1}$ in \eqn{eq:dss}. Using the
 usual OLS expressions for the partial regression, $\var(c) = s^2 (Z'Z)^{-1}$.
  Hence, we equate $(Z'Z)^{-1}$ with $\var(c)/s^2$. To get the White
 estimator that corresponds to $Z'Z$, we form the White estimate for $\var(c)$.
  The sandwich formula for the variance of the coefficients is
\begin{displaymath}
  \var(c) = (Z'Z)^{-1} Z'DZ (Z'Z)^{-1}, \quad D_{ij} = \delta_{ij} e_i^2\;.
\end{displaymath}
 Hence, the White estimator corresponding to $dSS$ in \eqn{eq:dss} replaces
$Z'Z$ with
\begin{equation}
 dSS_{white} = e'Z\left[\smfrac{1}{s^2}(Z'Z)^{-1} Z'DZ (Z'Z)^{-1}\right] Z'e  \;.
\label{eq:wdss}
\end{equation}
 This expression (and the resulting test) reduce to the usual sandwich test in
 the scalar case.  (The function {\tt white\_change\_in\_rss} passes the matrix
 shown in brackets in \eqn{eq:wdss} to the function {\tt change\_in\_rss}.)


\section{Tests in Logistic Regression} % --------------------------------------

 The current implementation resembles the protections offered in linear models,
 using the fact that a logistic regression is computed as a weighted least
 squares.  The distinction is with the White estimator: we use a Bennett bound
 for logistic regression.  The weights are used from the {\em prior} estimates
 rather than endogenously.  As in linear models, the variable {\tt protection}
 determines the nature of the testing procedure (really, the calculation of the
 p-value).  

 Given a set of explanatory varibles in the model $X$, the logistic regression
 solves the nonlinear estimating equations for $b$,
\begin{equation}
  X'y = X'p, \quad \mbox{ where } p = \frac{1}{1+\exp(-X'b)} \;.
\label{eq:esteq}
\end{equation}
 Note the similarity to the estimating equation (\aka, normal equation $X'y =
 X'(Xb)$) used in linear equations; in both cases, we choose $b$ so that the
 estimated values (either the probabilty $p$ or the fit $Xb$ reproduce the
 correlation of $Y$ with the explanatory variables.  

 Given a set of explanatory variables, we solve \eqn{eq:esteq} by using
 iteratively reweighted least squares.  The idea is a multivariate version of
 using a linear approximation to find the zero of a function.  Assume $f$ is an
 arbitrary function and we'd like to find a point $x^{*}$ for which $f(x^{*}) =
 0$.  Our current guess is $x_0$. Then a linear Taylor approximation gives us
\begin{displaymath}
  f(x) \approx f(x_0) + f'(x_0)(x-x_0)
\end{displaymath}
implies that (since $f(x) = 0$)
\begin{equation}
  x = x_0 + \frac{f(x)-f(x_0)}{f'(x_0)} = x_0-\frac{f(x_0)}{f'(x_0)} \;.
\label{eq:newton}
\end{equation}
 For logistic regression, $f$ is the derivative of the log of the likelihood.
  Let $p$ denote the estimated probabilities from the current fit; define the
 associated variance estimates as the diagonal of the matrix $V$,
\begin{displaymath}
  V_{ii} = p_i (1-p_i), \quad i=1,\ldots, n\;.
\end{displaymath}
 The estimating equation \eqn{eq:esteq} is the first derivative of the log of
 the likelihood function.  Deviations of $f(b) = X'(y-p_b)$ at the current
 solution mean that we need to change $b$ to improve the solution. The second
 derivative of the log-likelihood (which is the slope of the first derivative)
 is the matrix $X'VX$.  Hence, following the script of \eqn{eq:newton} we have
 the classic Newton iteration
\begin{equation}
  b_1 = b_0 - (X'V_0X)^{-1}X'(y-p_0) \;,
\label{eq:newton}
\end{equation}
 where $V_0$ is the diagonal matrix computed from $p_0 = 1/(1+e^{-x'b_0})$,
 which is the estimated probability at $b_0$.


 Rather than directly follow this Newton iteration, we use weighted least
 squares.  At the convergence of Newton's method, we reach a fixed point for
 which
\begin{displaymath}
  b = b - (X'VX)^{-1}X'(y-p) \;.
\end{displaymath}
 In order to write this as a (weighted) least squares, note that the associated
 variance matrix appears twice in the general form of a GLS estimator:
\begin{displaymath}
  \hat\beta = (X'WX)^{-1}X'WY \;.
\end{displaymath}
 We need to squeeze another $V$ into the Newton step \eqn{eq:newton}.  We can
 write this as a weighted least squares by forming a so-called pseudo-response
 $y^{*} = Xb + V^{-1}(y-p)$. We then compute the next iteration as
\begin{equation}
  b_1 = (X'V_0X)^{-1}X'V_0y^{*} \;.
\label{eq:irls}
\end{equation}

  For testing, however, the weights are fixed at the values obtained by the
 current model only, without an update for the variable underconsideration.


 This is in keeping with the second level of protection used in the linear
 model.  We have found this sufficiently useful in logistic regression as to
 omit the more traditional updating.  (It also keeps the code much simpler.)
  The key function in the interface is {\tt add\_predictors\_if\_useful}.
 

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the Wald $F$ statistic for the improvement in
 the model.  We prefer the Wald statistic (based on a change in the
 log-likilihood) to the score test (the coefficient estimates) to avoid problems
 in which the slope is poorly determined but highly useful (\eg, a separating
 hyperplane -- or nearly so -- has been found.

\item Use Bennett's inequality.  {\bf the current version only works for one
 dim}.  The values $y_i$ of the response are assumed to be bounded within the
 range $m \le y_i \le M$.


\end{enumerate}




\end{document}
