% -*- mode: LaTex; outline-regexp: "\\\\section\\|\\\\subsection";fill-column: 80; -*- 
\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{amssymb}
\usepackage{graphicx}

\input{/Users/bob/work/papers/standard}

\newtheorem{assumption}{Assumption}

% --- Paragraph split
\setlength{\parskip}{0.00in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.4}

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\small Implementing the Auction}}}
\newcommand{\eps}{\epsilon}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}



% --- Title

\title{  
         Implementing the Auction
}

\author{
        Robert A. Stine                                      \\
        Department of Statistics                             \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          \\
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle 

\vspace{-0.5in} \centerline{\bf Abstract} 

\clearpage

\section*{Reading and handling data}  % ----------------------------------------

 The first step of data processing is to convert everything into numbers.  The
 code that runs the auction only accommodates numerical variables.  Everything
 -- missing data, categorical variables -- must be represented numerically.  The
 program {\tt csv\_parser} handles this chore.  (The file {\tt csv\_parser.cc}
 is kept in the {\tt boost} directory.)  This command line tool converts data
 given in a csv file into the format used in the auction.  The most useful
 application of {\tt csv\_parser} puts files representing the variables into a
 named directory along with a shell script {\tt index.sh} for writing the data
 in streaming style.


\subsection*{Parsing a csv file}

 The first line of the input csv file holds the names of the variables and any
 descriptive information, basically the sort of named columns used by JMP or R.
  Names can consist of characters, numbers, spaces and the characters:
 \begin{verbatim} ., _, [, ], / \end{verbatim}
 Others might ought to be added the in later parsing of variables.  If the same
 name is used for two (or more columns), the second occurance will have ``\_2''
 appended, then ``\_3'', and so forth for others.

 The first input column is used to identify an indicator for cross-validation.
  If the first input column is a list of the labels ``in'' and ``out'', then
 this variable will be flagged as indicating which cases to use in the model,
 and which to reserve for validation.  Rather than generate 2 indicators (one
 for every symbol as done for explantory categorical variables), it will only
 generate 1.  These boolean variables will be placed first in the file sent to
 the auction for modeling with a flag to be used for validation rather than as a
 response or explanatory variable.  In the auction, only those cases marked "in"
 will be used for estimation.  All cases will be predicted.

 In addition to supplying names, this first line also assigns attributes to
 variables.  This is done by listing name=value pairs of attributes within
 curly brackets and separated by semi-colons.  No spaces are allowed within the
 attributes.  Currently used attributes defined by the software are

 \begin{tabular}{cl}
   role   &  One of ``x'', ``y'' or ``context'' (defaults to x).     \cr
   stream &  Identifies a specific input stream  (defaults to main). \cr
 \end{tabular}

 \noindent
 You must make one variable the response, or the software won't know which to
 use.  The default stream option is for convenience since it avoids having to
 use the bracketed option {stream=main} for every variable in the file.
 As an example, an input csv file with 3 variables might begin as follows
 \begin{verbatim}
      Var1, a/b, Var.3{stream=sub;priority=2;knots=4}
       1, 2, 3
       3, 4, 5
       a,  , 5
 \end{verbatim}
 The presence of a non-numerical symbol (in the example, the 'a' in the 3rd row)
 in the data for Var1 converts Var1 into a categorical variable. Every unique
 value found in this column will cause the software to generate an indicator
 (so, you'll get a lot of these if this was an accident and the column really is
 numerical).

 Identify missing data in the csv file by empty fields, as shown on the last
 line of the prior example.  (You don't need to leave a blank space; ``,,''
 would work on the third line of the example just as well.  {\tt csv\_parser}
 automatically builds an indicator for any variable that has missing data and
 fills the missing values with the mean of the observed values.

 Known limitations:
 \begin{itemize}
   \item No blanks at the end of the line are allowed!
   \item  You need a *mix* of in/out for the leading indicator (not all in).
 \end{itemize}
   
\subsection*{Output of the csv parsing}

 The parsed data are written into a named directory (which is creatded if it
 does not exist).  The key file in this directory is a shell script named {\tt
 index.sh}.  Executing this script produces the streaming style input expected
 by the auction.  You can edit this script to produce test input, shorter data
 series, or limit the number of variables.  Three lines in the script define the
 data for each variable:
\begin{enumerate}
    \item  name of the variable (square brackets denote an indicator)
    \item  attributes for this variable as space delimited strings/numbers
    \item  command that sends the variable to standard output (usually {\tt
           echo}.
\end{enumerate}  
 As in JMP, square brackets in the name of a variable identify an indicator for
 a category. 

 As a last step, it will be convenient to later stream the data into the auction
 by cat'ing a compressed version of the data.  To build this file, make the
 shell script executable ({\tt chmod +x index.sh}) and then run the script with
 output to a file, {\tt .\/index.sh > data}).  Then compress the data using gzip
 into {\tt data.gz}.  When the auction is run below, simply cat the compressed
 data via stdin to the auction.  Several examples in the makefile in auctions
 illustrate how this works.


\subsection{Data processing within the Auction} 
 

 Numerical data is read from a file aud converted into two vectors of {\tt
 Column} objects.  The read step is done by the routine {\tt
 insert\_columns\_from\_file} called from the main function that sets up the
 auction.  A {\tt Column} is a wrapper (envelope) class for the column data
 object that holds the actual data; this avoids the overhead of copying.  Column
 objects are lightweight.  The {\tt ColumnData} object just reads numbers
 (doubles, in particular), then computes summary characteristics (including the
 number of unique values and means) from the input values in {\tt
 init\_properties}.  It knows nothing about the interpretation of names or
 properties.  As far as a {\tt Column} is concerned, names and descriptions are
 simply strings.  (The only conversion that happens in columns is to fill spaces
 in names with the underscore character, converting for example ``the var'' into
 ``the\_var''.)

 The auction code carefully examines the names of the leading input variables to
 determine how to use the supplied cases.  Cases may be of three types:
\begin{itemize}
\item Missing cases.  These supply information about possible explanatory
 variables, but the not the response.  These are not predicted and are only used
 for information about features (such as lagged variable values).  These do not
 appear in the saved data file produced at the end of an analysis and do not
 appear in the data object ({\tt gsl\_data}) that supports the regression model.
\item In-sample or estimation cases.  These are complete $(y_i,\xx_i)$ tuples
 used to identify the model.
\item Out-of-sample or validation cases. These are complete cases, but will not
 be used in the fitting process.  They only appear for calculation of
 predictions for validation purposes.
\end{itemize}
 The input command-line argument ``x'' denote the number of leading cases which
 are formally missing the response.  These {\em must} be the leading cases in
 the data file.  These cases are neither used in fitting nor validation; they
 are simply there for building predictive features.  These rows are skipped over
 when transferring the response ({\tt build\_model\_data}) and accepted features
 into the auction.  (Each expert knows rows to skip in the model. See the
 function {\tt convert\_to\_model\_iterators} defined for the ExpertABC.)  If
 the leading column is named ``[in/out][in]'', then the first data column is
 treated as a boolean indicator of the in-sample and out-of-sample cases.  (For
 extra cases, the values of any response column -- indicator and numerical value
 -- are ignored.)


 The parsing of the names of variables and the descriptions (property lists)
 happens when the columns are convered into features.  Features are initialized
 from columns.  As with columns, a feature is a wrapper hiding the particular
 type of variable.  {\tt ColumnFeatures} hold a column; other types, such as
 unary features, hold a function that converts the values in a another feature.
  The function {\tt add\_attributes\_from\_paired\_list}, for example, is called by
 a column feature to convert the ``description string'' read by a column into
 the paired property list used by a feature.  Columns, for example, might simply
 use this string as purely documenation, as in free text.  Features, in
 contrast, expect to find that this string has a paired list of terms.

 Columns and Features live separately from the code that actually fits the
 regression. The GSL routines, such as those in {\tt gsl\_regr}, work with a
 distinct data object, essentially a plain old C array.  The reason for this
 duplication is to allow outside routines, such as features, to have a mix of
 validation and estimation data, whereas the data inside the regression model is
 {\em only} that which is used for fitting the model.  Also, this separation
 makes it easy to handle the use of split samples for validation. Code for {\tt
 for} loops needs to be able to iterate over the elements of the data very
 quickly, so we pack the estimation cases adjacent to each other by permuting
 the input data.  The validation cases are sorted away from the estimation cases
 in the GSL objects, whereas they might be mixed together in the features.



\section{Experts} % --------------------------------------------------------------------------

 The auction uses several types of experts.  (In the code, these are identified
 by an enumerated type.)  Experts provide a way to search in a more 'depth
 first' rather than 'breadth first' manner.  With experts, one does not have to
 examine, for example, every linear effect before considering a second-order
 effect.

 {\bf Reserved Stream Names} There are two reserved stream names, both
 distinguished by being in all caps.  The default stream name is ``MAIN''.  The
 other reserved name is ``LOCKED''.  Variables in the locked stream are forced
 into the model at the time of initialization.  Since all of these variables
 will appeaer in the list of model features, they will automatically be
 considered in interactions with several other experts.  Context variables do
 not go into any stream; they are reserved for the context of the model.

\begin{description}

\item[Source experts] produce new features for consideration; in general, source
 experts build variables from input data.  The features from a source expert
 could be generated from an algorithm or some other mechanism, but not from the
 progress of the auction itself.  Each problem generally has several source
 experts (as identified by the 'stream' attribute in the data file).  Splitting
 the data into several sources allows the auction to discover collections of
 variables of more interest rather than having to proceed through them all at
 once.  Source experts are assigned $\alpha$-wealth at the time that the auction
 is initialized.  Source experts continue to offer features until either all of
 the associated features are accepted into the model or every feature has been
 tried since the last variable was added to the model.  Every unused feature is
 tried at least once after the last added variable.  If on this last pass, every
 variable is rejected, then the expert no longer offers other variables.

 Our implementation also generates interactions {\em within}
 the features of stream.  If $X_1$ and $X_2$ are from stream {\bf A} and $Z_1$
 and $Z_2$ are from stream {\bf B}, then the corresponding source experts will
 offer squares of the $X$s and $Z$s, $X_1*X_2$ and $Z_1*Z_2$, but not
 cross-products such as $Z_1*X_2$.  (Those can be formed using the
 'interacts\_with' attribute of a feature, but is not routinely done.)

\item[Parasitic or scavenger experts] (or perhaps more politely called derived
 experts) construct features from those that have already made an appearance in
 the auction.  These might be, for instance, various spatial interactions of a
 feature that was initially rejected from the auction previously.  Some versions
 of the auction have used parasites that form interactions among variables in
 the model; these are now handled elsewhere.  The idea is that parasites ought
 to be the ``voice of the forgotten'' or rejected features.  Parasites obtain
 $\alpha$-wealth from a tax on bids.  The tax on bids is small since we expect a
 lot of bids relative to the number of accepted features.

\item[Custom experts] are spawned when a feature is added to the model.  By
 default, the addition of a feature to the model generates an expert that offers
 an interaction in that feature and any other features in the model.  The
 feature may itself add other experts as well.  For example, a feature might add
 time lags segmentation of itself (to allow spatial heterogeneity, for
 example), or perhaps specify interactions with a specific collection of
 features.  Custom features allow the original expert to focus on a simple task,
 say picking features from a large data source, rather than having the
 distraction of following the status of features that join the model.  Custom
 experts are funded from the pay-out received when a variable joins the model.

 As an example, one might have introduce a ``knot expert'' that offered possible
 places to segment the effect of a feature (think turbo smoothing).  This expert
 would be bound to a specific feature accepted into the model.  Because the
 expert is tied to one specific feature, it is funded from that feature.
  Suppose that the expert $\cal F$ recommends the feature $X$ to the auction,
 placing some bid amount $b$.  The variable $X$ joins the auction, earning
 pay-out $\omega$.  Some fraction of this pay-out goes directly back to the
 recommending expert $\cal F$, the rest of the pay-out goes to experts related to
 $X$.  By default, $X$ is bound to an expert that builds interactions of $X$
 with other features in the model.  Assume that $X$ also comes with a custom
 expert $\cal E$ that segments $X$ in the model, allowing nonlinear effects for
 $X$.  $\cal E$ unlike parasitic experts, gets created and funded from a tax on
 the pay-out on $X$ when $X$ is accepted.\footnote{The code that builds custom
 experts is in {\tt auction.Template.h}, specifically {\tt
 pay\_winning\_expert}.  Any feature added to the model automatically creates an
 expert that builds interactions among variables added to the model. In
 addition, a feature $X$ that has the attribute {\tt interact\_with} will also
 generate an interaction between $X$ and all features that have an attribute
 named by {\tt interact\_with}.  For example, if {\tt interact\_with} has value
 {\tt quarter}, then $X$ will be crossed with any feature that has the attribute
 {\tt quarter}.}

\item[Calibration experts] serve a very narrow role of calibrating the fit of
 the current model.  These are funded at initialization and operate by-and-large
 outside the usual auction process.  They are experts more in name alone to
 unify the coding.
\end{description}


 It is worthwhile to review the funding of experts.  The current algorithm funds
 experts using ``taxes'' in order to avoid adding further wealth beyond the
 initial wealth allocated at initialization.  The auction funds source during
 the initialization process, allocating the total initial $\alpha$-wealth given
 to the auction among them (typically equally).  (Calibration experts are also
 funded at initialization, but out of a separate pool of wealth.)  The auction
 funds parasitic experts during the bidding process.  Because parasitic experts
 bid on features that are rejected, these are funded by a ``tax'' on bids placed
 in the auction.  For example, suppose a source expert $\cal E$ bids $\alpha =
 0.01$ for the feature $X$. The auction taxes this bid at, say, 5\% and
 distributes the tax revenue $0.01 \times 0.05 = 0.0005$ among those experts
 that bid on rejected features.  The auction funds custom experts that bid on
 accepted features by taxing (at some possibly different -- and higher -- rate)
 the pay-out $\omega$ made to an expert when a feature is accepted into the
 model.

\clearpage
\section{Steps in the Auction} % -------------------------------------------------------------

 This section describes the generation of experts in the auction and their
 initial funding. These all basically use the universal bidding rule based on a
 Cauchy distribution, with the added ability to ``spend it all'' before running
 out of features.

 {\em Prior to the start of the auction}, the software creates an initial set of
 experts.  These derive from the structure of the input explanatory variables
 and chosen program options.  The first experts are the parasite/scavenger
 experts:
 \begin{description}
  \item[Rejected feature polynomials.] This expert builds polynomials in
 rejected features to see if some simple nonlinearity might be present.  If $X$
 has been rejected, then the cubic version of this expert offers the bundle $X$,
 $X^2$ and $X^3$ of three features to the auction.  (If the linear term $X$ is
 in the current model, the expert offers only $X^2$ and $X^3$.)
  \item[Accepted/Rejected feature interactions.] This expert interacts features
 that were rejected by the auction with those that were accepted.

 \end{description}
 These receive no initial alpha-wealth.  They only get funded from taxes applied
 to bids placed on variables considered for the auction.

 The remainiong initial experts are ``source experts'' that recommend new
 variables (basically, data read from an external file).  For each named input
 stream (the default is named ``main'' and it does not use the context stream),
 the auction adds two experts:
 \begin{description}
  \item[Linear feature.] Recommends the actual input feature, as is.  The expert
 guarantees to make two passes over the list of variables in this stream before
 it spends down all of its allocated alpha-wealth.
  \item[Interactions.]  This expert recommends interactions among the variables
 within the stream. 
 \end{description}
 These are funded by allocating the initial alpha-wealth of the auction, with
 52\% of the wealth given to a stream going to the linear, and 48\% going to the
 interactions.  The difference makes sure we try linear terms before going after
 interactions.  (The associated code is in the file {\tt auction.test.cc} where
 a comment indicates {\tt Assembling experts}.)  In addition, the software will
 add a calibration expert.  Also, the software can add principal components or
 RKHS basis vectors.

 {\em Dynamic experts join the auction as the modeling continues.}  The auction
 adds additional experts ``on the fly'' when a variable gets added to the model.
  If the added variable has the attribute {\tt interact\_with\_parent}, then the
 auction gains an expert that will interact the newly added variable with any
 variable from the named stream. (??? How does it use those that have already
 been tried?)  If the added variable has the integer-valued attribute
 ``max\_lag'', then an expert will offer up to this many lags of the added
 variable.  Finally, every added variable gets an expert that interacts it with
 every other variable already in the model.  Notice that this expert overlaps
 with the source expert when the variables come from a common stream.  For
 example, if $X_1$ and $X_2$ come from the stream {\tt main} and first $X_1$ and
 then $X_2$ are added to the model. The source stream for {\tt main} will offer
 the interaction of these, and the dynamic stream will also recommend their
 interaction.  (That could be easily handled if the two experts bid for $X_1 *
 X_2$ at the same time, but results in some wasted alpha if they win separate
 bids.)

 All of these dynmically-added experts are funded from a tax on the payout when
 the variable is added.  (See the code for {\tt pay\_winning\_expert} called by
 {\tt auction\_next\_feature} in the file {\tt auction.Template.h}.)  Many of
 these experts bid only on a small list of features, meaning that they often run
 out of features quickly.  They can pass over them more than once, but in the
 current implementaition experts with no features to recommend (now or in the
 future) get harvested.  Any remaining alpha is collected by the auction.


\section{Tests in Least Squares} % -----------------------------------------------------------

 The value of the variable {\tt protection} (which is set in the constructor of
 an auction) determines how the code treats the various test statistics.  The
 higher the level of the protection, the more conservative the computation of
 the p-value associated with a test.  The current implementation uses the
 following methods, assuming that the current model includes $k$ predictors
 with an intercept and has residuals in the $n$-vector $e$.  Consider testing
 the effect of adding $k_{new}$ predictors that produce residuals $e_{new}$.

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the usual $F$ statistic for the added
 variable(s),
\begin{equation}
    F_0 = \frac{ dSS/k_{new} }{(e'e-dSS)/(n-1-k-k_{new})} \;.
\label{eq:f0}
\end{equation}
 Obtain the p-value by comparing $F_0$ to the $F$-distribution with $k_new$ and
 $n-1-k-k_{new}$ degrees of freedom.  Equation \eqn{eq:dss} gives the familiar
 expression for the change in the regression sum-of-squares obtained by adding
 the additional variables.

\item White estimator, standard $F$ test. In this case, the White estimator is
 used to compute the regression sum-of-squares, with an ``F-like'' statistic
\begin{equation}
    F_1 = \frac{ dSS_{white}/k_{new} }{(e'e-dSS_{white})/(n-1-k-k_{new})} \;.
\label{eq:f1}
\end{equation}
 Expression \eqn{eq:wdss} gives the White estimator used in the numerator of
 $F_1$.  The denominator of $F_1$ matches that of $F_0$.  As before, obtain the
 p-value by comparing $F_1$ to the $F$-distribution with the indicated degrees
 of freedom.

\item White estimator, conservative $F$ test. The White estimator is again used
 to find the regression SS, but now the denominator uses the residual sum of
 squares {\em without} adding the new variable,
\begin{equation}
    F_2 = \frac{ dSS_{white}/k_{new} }{e'e/(n-1-k-k_{new})} \;.
\label{eq:fstat}
\end{equation}
 Calculate the p-value as for $F_1$.

\item Conservative p-value.  Not yet implemented.  The idea is to use a more
 robust procedure to find the p-value in place of the F-distribution.  I guess I
 was thinking of something like a Bennett p-value here applied to the White F
 stat.  That seems way too conservative at this point, and it probably makes
 more sense to adjust for possible dependence using a block calculation.

\end{enumerate}

\subsection{White variance estimates} % ---------     ----------     ---------

 Once the protection level in a least squares regression is larger than 0, we
 use a White (or sandwich) estimator to find the numerator of the $F$-statistic
 that is used to measure the statistical significance of an added variable(s).

 The White estimate is generally proposed as a method of estimating a
 heteroscedastic-robust standard error.  It is used in finding a more robust
 t-statistic.  Consider the impact of adding one or more new features $X_{new}$
 to a regression model that already contains the variables $X$ and has residuals
 $e$. The first stage of testing sweeps out the prior explanatory variables $X$
 from $X_{new}$, forming $Z$ defined as (Dean and Dong Yu omit this for speed.)
\begin{displaymath}
  Z = (I-X(X'X)^{-1}X') X_{new} = (I-H_X) X_{new}  \;.
\end{displaymath}


 As motivation, consider testing the value of adding only one new feature, which
 after sweeping out $X$ is the column vector $z$.  The slope for the added
 variable is $\hat\gamma = (z'e)/(z'z)$.  The White estimate of the variance of
 $\hat\gamma$ is
\begin{displaymath}
  \var_w(\hat\gamma) = \frac{\sum_i (z_ie_i)^2} {(z'z)^2} 
   = \frac{z'E^2z} {(z'z)^2} \;,
\end{displaymath}
 where $E$ is an $n \times n$ diagonal matrix with elements $E_{ii}=e_i$.  (Note
 that we use the residuals from the fit with $X$ alone, {\em not} both $X$ and
 $z$. See \citet{fosterstine04:bank} for an explanation of this choice.)  Hence, the
 square of the robust $t$-statistic is
\begin{equation}
  t^2 = \frac{\hat\gamma^2}{\var_w(\hat\gamma)} 
      = \frac{(z'e)^2}{z'E^2z} \;.
\label{eq:t2}
\end{equation}
 Alternatively, we can obtain the same test statistic using the approach found
 in building an $F$-statistic.  This will be useful since we will consider
 adding bundles of several features, and we will test these bundles as a group
 rather than one at a time.  Generally, an $F$-statistic in regression is the
 ratio the change in the regression mean-squares to the residual mean-squares.
  When adding $z$ to the model, the regression sum-of-squares increases by
 $\hat\gamma' z'z \hat\gamma$.  We need to change this expression to obtain an
 $F$-statistic that corresponds to the robust $t$-statistic.  In particular,
 think of $z'z$ in $\hat\gamma' (z'z) \hat\gamma$ as proportional to the inverse
 of the variance of $\hat\gamma$.  We then replace the usual least squares
 expression by that provided by the White estimator.  The resulting $F$-like
 statistic replaces
\begin{equation}
  \frac{\hat\gamma' (z'z) \hat\gamma}{(e'e)/(n-q)}=
  \hat\gamma\left(\frac{z'z}{(e'e)/(n-q)}\right) \hat\gamma 
\end{equation}
by
\begin{equation}
  \hat\gamma' (\var_w(\hat\gamma))^{-1} \hat\gamma = 
  \left(\frac{z'e}{z'z}\right) \left(\frac{z'E^2z}{(z'z)^2}\right)^{-1}
  \left(\frac{z'e}{z'z}\right) = \frac{(z'e)^2}{z'E^2z} \;,
\label{eq:F2}
\end{equation}
 which matches the squared robust $t$-statistic in \eqn{eq:t2}.


 In the case of several added variables, we need to form the corresponding
 matrices.  The calculations are more messy, but follow the same steps.  We
 start by finding the increase in the regression sum-of-squares.  Regress the
 current residuals $e$ on $Z$, obtaining the partial coefficients $\hat\gamma =
 (Z'Z)^{-1}Z'e$.  The increase in the regression sum-of-squares due to this
 partial regression is then the quadratic form
\begin{eqnarray}
   \hat\gamma'(Z'Z)\hat\gamma \;,
\label{eq:dss}
\end{eqnarray}
 and the $F$0-statistic normalizes this expression by an estimate of the error
 variation.\footnote{The method {\tt change\_in\_rss} in gsl\_regr computes
 $\Delta SS$ for the current regression.}  The associated $F$-statistic for
 adding $m$ new features to the model is
\begin{equation}
  F = \frac{\Delta SS}{m \; s^2} 
    = \frac{1}{m} \; \hat\gamma' \var(\hat\gamma)^{-1} \hat\gamma \;,
\label{eq:F}
\end{equation}
where, if we continue to conservatively estimate the scale, $s^2 = (e'e)/(n-q)$.


 To obtain the White version of the $F$-test, replace the variance estimate in
 \eqn{eq:F} with the corresponding White estimator.  The sandwich formula for
 the variance of the coefficients is
\begin{displaymath}
  \var_w(\hat\gamma) = (Z'Z)^{-1} Z'E^2Z (Z'Z)^{-1}, 
  \quad E_{ij} = \delta_{ij} e_i\;.
\end{displaymath}
 Hence, the White version of the $F$-statistic \eqn{eq:F} is
\begin{eqnarray}
  F_w
 &=& \smfrac{1}{m} \;
     e'Z(Z'Z)^{-1}\left[(Z'Z)^{-1}Z'E^2Z(Z'Z)^{-1}\right]^{-1}(Z'Z)^{-1}Z'e \cr
 &=& \smfrac{1}{m} \; e'Z (Z'E^2Z)^{-1} Z'e 
\label{eq:Fw}
\end{eqnarray}
 (Compare to the scalar version \eqn{eq:F2}.)\footnote{The function {\tt
 white\_change\_in\_rss} passes the matrix shown in brackets in \eqn{eq:wdss} to
 the function {\tt change\_in\_rss}.}

\subsection{White with Blocks}

 Suppose that we treat the observations as possibly dependent in blocks.  That
 is, assume that the data $Y' = (Y_1', Y_2', \ldots, Y_K')$ come in $K$ clusters
 with covariance matrix $V_k$ within the $k$th group:
\begin{displaymath}
  Y = \mu + \epsilon, \quad \Var(\epsilon) = 
     \left(
     \begin{array}{ccccc}
       V_1     & 0      & \cdots & 0      \cr
        0      & V_2    & \cdots & 0      \cr
       \vdots  & \vdots & \ddots & \vdots \cr
        0      & 0      & \cdots & V_K   
     \end{array}  
     \right)
\end{displaymath}
 The usual White estimator is robust to heteroscedasticity, but not
 dependence. In this case, such as longitudinal data, we form a blocked White
 estimator. 

 For convenience, suppose that each of the $K$ groups is of size $b$ so that $n
 = K b$.  The blocked White estimate of the variance of the least squares
estimates $\hat\beta$ based on a collection of $q$ features $X$ is
\begin{equation}
  \var(\hat\beta) = (X'X)^{-1} \left(\sum_k X_k'e_ke_k'X_k\right) (X'X)^{-1},
\label{eq:blockwhite}
\end{equation}
 where $e_k$ are the residuals within the $k$th block.  To express these more
 directly, let $B_k$ denote an $b \times n$ matrix with an $b\times b$ identity
 matrix $I_b$ positioned to extract rows associated with block $k$:
\begin{equation}
  B_k = \left(
   \begin{array}{ccccccc}
     0  & \cdots & 0 & \underbrace{I_b}_{\mbox{\scriptsize block $k$}} & 0 & \cdots & 0 
   \end{array}
   \right) \;.
\label{eq:Bk}
\end{equation}
 If $e = Y - X \hat\beta$ denote the residuals from the current fit, then $B_ke
 = e_k$, the residuals in block $k$.  The expression for the blocked variance
estimate \eqn{eq:blockwhite} then becomes
\begin{eqnarray}
  \var(\hat\beta) 
  &=& (X'X)^{-1} \left(\sum_k X_k' B_k e \, e' B_k' X_k\right) (X'X)^{-1} \cr
  &=& (X'X)^{-1} \left(\sum_k X_k' B_k (I-H)YY'(I-H) B_k' X_k\right)(X'X)^{-1}.
\label{eq:blockwhite1}
\end{eqnarray}
 If $\ev Y = \mu$ lies in the column span of the current features $X$, then $H
 \mu = \mu$ so that $(I - H)Y = \epsilon$, and the variance expression becomes
\begin{eqnarray}
  \var(\hat\beta) 
  &=& (X'X)^{-1} \left(\sum_k X_k' B_k \eps \eps' B_k' X_k\right)(X'X)^{-1}\cr
  &=& (X'X)^{-1} \left(\sum_k X_k' \eps_k \eps_k' X_k\right)(X'X)^{-1}
\label{eq:blockwhite2}
\end{eqnarray}
 Taking expected values, the estimator is unbiased for the variance of the
 estimator (ignoring issues of selection, of course)
\begin{equation}
  \ev \var(\hat\beta) 
  = (X'X)^{-1} \left(\sum_k X_k' V_k X_k\right)(X'X)^{-1}.
\label{eq:blockwhite3}
\end{equation}

 To gain some intuition about the blocked White estimator defined in
 \eqn{eq:blockwhite}, it is helpful to compare it to a direct estimator of the
 variance of the slope.  Let $\hat\beta_k$ denote the estimated slope computed
 within the $k$th block.  It is then possible to show that blocked White
 variance approximates the observed variance of $\hat\beta_k$ under some
 conditions \citep{fosterlin10}.  For convenience, assume that the features $X$
 are identical within each block,
\begin{displaymath}
   X = 
    \left( \begin{array}{c}
       X_1 \cr  X_2 \cr \vdots \cr X_K 
    \end{array} \right)
    = 
    \left( \begin{array}{c}
       X_1 \cr  X_1 \cr \vdots \cr X_1 
    \end{array} \right)
\end{displaymath}
 With this conformity, $\hat\beta_k = (X_1'X_1)^{-1}X_1'Y_k$, and the
 overall slope fit to all of the data is the average of the $\hat\beta_k$:
\begin{displaymath}
  \hat\beta = (X'X)^{-1}X'Y 
       = \frac{1}{K} \underbrace{(X_1'X_1)^{-1} \sum_k X_1'Y_k}_{\hat\beta_k}
       = (X_1'X_1)^{-1} X_1' \ol{Y} \;,
\end{displaymath}
 where the $b \times 1$ vector $\ol{Y}$ is the mean response under the repeated
 conditions. Since the first row, for example, of each block is the same, the
 estimator simply averages the first $y$'s and uses the average in the
 regression.

 This homogeneity shows up in the blocked White estimator as well. The variance
 estimate \eqn{eq:blockwhite} reduces to
\begin{eqnarray}
  \var(\hat\beta) 
   &=& \frac{1}{K^2}
       (X_1'X_1)^{-1} \left(\sum_k X_1'e_k e_k'X_1\right) (X_1'X_1)^{-1} \cr
   &=& \frac{1}{K^2}\sum_k (X_1'X_1)^{-1}X_1'(Y_k-X_1\hat\beta)
                           (Y_k-X_1\hat\beta)'X_1 (X_1'X_1)^{-1} \cr
   &=& \frac{1}{K^2}\sum_k (\hat\beta_k-\hat\beta)(\hat\beta_k-\hat\beta)'\cr
   &=& \frac{1}{K^2}\sum_k (\hat\beta_k-\ol{\hat\beta_k})
                           (\hat\beta_k-\ol{\hat\beta_k})'  \cr
   &=& \frac{K-1}{K^2} \var(\hat\beta_k) \;,
\end{eqnarray}
 as in $\var(\ol{x}) = s_x^2/n$.  The difference between the direct estimator
 and the blocked White estimator lies in the information matrix $X_k'X_k$ for
 the blocks.  The larger the differences among the blocks, the greater the
 difference between the direct and White estimates. \citet{fosterlin10} discuss
 these differences further.

\section{Tests in Logistic Regression} % -----------------------------------------------------

 The current implementation resembles the protections offered in linear models,
 using the fact that a logistic regression is computed as a weighted least
 squares.  The distinction is with the White estimator: we use a Bennett bound
 for logistic regression.  The weights are used from the {\em prior} estimates
 rather than endogenously.  As in linear models, the variable {\tt protection}
 determines the nature of the testing procedure (really, the calculation of the
 p-value).  

 Given a set of explanatory variables in the model $X$, the logistic regression
 solves the nonlinear estimating equations for $b$,
\begin{equation}
  X'y = X'p_b, \quad \mbox{ where } p_b = \frac{1}{1+\exp(-X'b)} \;.
\label{eq:esteq}
\end{equation}
 Note the similarity to the estimating equation (\aka, normal equation $X'y =
 X'(Xb)$) used in linear equations; in both cases, we choose $b$ so that the
 estimated values (either the probability $p$ or the fit $Xb$ reproduce the
 correlation of $Y$ with the explanatory variables.  (We subscript $p$ by the
 coefficient $b$ as a reminder of this dependence; later we simplify this to
 $p_1$ if the estimates are $b_1$. The same convention applies to other terms
 such as matrices constructed from the estimated probabilities.)

 Given a set of explanatory variables, we solve \eqn{eq:esteq} by using
 iteratively reweighted least squares.  The idea is a multivariate version of
 using a linear approximation to find the zero of a function.  Assume $f$ is an
 arbitrary function and we'd like to find a point $x^{*}$ for which $f(x^{*}) =
 0$.  Our current guess is $x_0$. Then a linear Taylor approximation gives us
\begin{displaymath}
  f(x) \approx f(x_0) + f'(x_0)(x - x_0)
\end{displaymath}
 implies that (since $f(x^{*}) = 0$ and we hope that the zero of $f$ is $\approx
 x^{*}$)
\begin{equation}
  x = x_0 + \frac{f(x)-f(x_0)}{f'(x_0)} = x_0-\frac{f(x_0)}{f'(x_0)} \;.
\label{eq:newton}
\end{equation}
 For logistic regression, $f$ is the derivative of the log of the likelihood.
  Let $p$ denote the estimated probabilities from the current fit; define the
 associated variance estimates as the diagonal of the matrix $V$,
\begin{displaymath}
  V_{ii} = p_i (1-p_i), \quad i=1,\ldots, n\;.
\end{displaymath}
 The estimating equation \eqn{eq:esteq} is the first derivative of the log of
 the likelihood function.  Deviations of $f(b) = X'(y-p_b)$ at the current
 solution mean that we need to change $b$ to improve the solution. The second
 derivative of the log-likelihood (which is the slope of the first derivative)
 is the matrix $X'VX$.  Hence, following the script of \eqn{eq:newton} we have
 the classic Newton iteration
\begin{equation}
  b_1 = b_0 - (X'V_0X)^{-1}X'(y-p_0) \;,
\label{eq:newton}
\end{equation}
 where $V_0$ is the diagonal matrix computed from $p_0 = 1/(1+e^{-x'b_0})$,
 which is the estimated probability at $b_0$.


 Rather than directly follow this Newton iteration, we convert the estimation to
 a regression problem so that we can use weighted least squares.  (This allows
 us to use the same code base for fitting linear and logistic regression
 models.)  Convergence of Newton's method produces a fixed point $b$ that
 satisfies
\begin{displaymath}
  b = b - (X'VX)^{-1}X'(y-p) \;.
\end{displaymath}
 (Note: this expression suggests a way to form the sandwich estimator.) In order
 to write this as a (weighted) least squares, note that in the usual expression
 for a GLS estimator, the variance matrix appears twice:
\begin{displaymath}
  \hat\beta = (X'W^{-1}X)^{-1}X'W^{-1}Y \;.
\end{displaymath}
 We need to squeeze another $V$ into the Newton step \eqn{eq:newton}.  We can
 write this as a weighted least squares by forming a so-called pseudo-response
\begin{equation}
 y^{*} = Xb + V^{-1}(y-p) \;.
\label{eq:ystar}
\end{equation}
  We then compute the next iteration as
\begin{equation}
  b_1 = (X'V_0X)^{-1}X'V_0y^{*} \;.
\label{eq:irls}
\end{equation}
 Notice that the variance matrix $V_0$ appears in the expression for $b_1$
 without an inverse that one might expect from the corresponding expression for
 the WLS estimator.  The explanation is the formation of the pseudo-response.
  If we treat $Xb$ on the r.h.s. of \eqn{eq:irls} as fixed, then heuristically
 $\var(y^{*}) = V^{-1}V V^{-1} = V^{-1}$ so $V_0$ {\em is} the inverser of the
 variance matrix of $y^{*}$.


 For testing, however, the weights are fixed at the values obtained by the
 current model only, without an update for the variable under consideration.
  This is in keeping with the second level of protection used in the linear
 model.  We have found this sufficiently useful in logistic regression as to
 omit the more traditional updating.  (It also keeps the code much simpler.)
  The key function in the interface is {\tt add\_predictors\_if\_useful}.
 

\begin{enumerate}
\setcounter{enumi}{-1}

\item No protection.  Construct the Wald $F$ statistic which is the
 change in the log of the likelihood for the improvement in the model.
  We prefer the Wald statistic to the score test (basically, $t$
 statistics for the coefficient estimates) to avoid problems in which
 the slope is poorly determined but highly useful (\eg, a separating
 hyperplane -- or nearly so -- has been found).

\item Use Bennett's inequality.  {\bf the current version only works
 for one dim}.  The values $y_i$ of the response are assumed to be
 bounded within the range $m \le y_i \le M$.  If there's only one
 explanatory variable, the expression for the new slope introduced
 into the logistic regression by adding the new variable $x_new$
 becomes (simplifying the formula \ref{eq:irls})
\begin{displaymath}
  b_{new} = \frac{z'y}{z'z}
\end{displaymath}
 where $z$ and $y$ have been swept of the current explanatory variables in the
 weighted inner product define by the current variables.  That is, express the
WLS \eqn{eq:irls} estimate as the solution to an OLS with the weights
incorporated into the variables 
\begin{equation}
  b_1 = (X_v'X_v)^{-1}X_v'y^{*}_{v}, 
    \quad X_v = V_0^{1/2}X, y^{*}_v = V_0^{1/2}y^{*} \;.
\label{eq:wls}
\end{equation}
 Alternatively, return to the Newton iteration shown in \eqn{eq:newton}.  One
 need only test the size of the increment $(X'V_0X)^{-1}X'(y-p_0)$ in order to
 decide whether to consider adding the proposed variable.

\end{enumerate}

\subsection{Comments on the implementation}

 The same code base handles both linear and logistic regression.  At the top
 level in {\tt gsl\_model}, both linear and logistic regressions inherit from the
 {\tt gsl\_regression} object.  The distinction likes in the default choice of
 the ``engine'' that handles the details of the calculations.  Specifically,
 engines are distinguished by how they define the inner product for all
 calculations. (All regressions rely on the underlying model for inner products
 such as matrix multiplication, inversion, and solving systems of equations.)
  For a linear model, we use a standard engine that has the common dot-product
 $\ip{x}{y} = \sum_i x_i y_i = x'y$.  For a logistic regression, we use a
 weighted dot product $\ip{x}{y} = \sum_i x_i y_i w_i$.  The added complication
 is that the weights change in a logistic regression; hence, there are
 convenience functions that ``reweight'' the engine.  Engines in the code (see
 gsl\_engine.h) do not inherit from a common base (we never need a list of
 engines); rather they implement common {\em policies}.  

 Functions handled by engines include those that seem clear ({\tt average,
 sum\_of\_squares, blas\_ddot}) as well as the otherwise innocuous functions
\begin{itemize}
  \item {\tt prepare\_vector\_for\_analysis}
  \item {\tt insert\_analysis\_matrix}
\end{itemize}
 Preparing a vector is a simple copy (OLS engine) or a copy plus scaling by the
 square root of the weights (for the WLS engine, as seen in the expression for
 the estimation shown in \eqn{eq:irls} above).  Inserting a matrix copies the
 matrix and scales the columns by the square root of the current weights
 $\sqrt{w_i}$.


 When a weighted engine is configured, the input includes a data object (one
 that supports the required data policy).  The weighted engine then extracts a
 {\em pointer} from the data object to the initial weights to use in
 calculations.  Changes to these weights affect subsequent calculations, but
 changes should be accompanied by telling the engine ({\tt
 weights\_have\_changed}).


 When a candidate feature is offered to a logistic model via {\tt
 add\_predictor\_if\_useful}, the following steps occur: (Multiple features are
 handled similarly.)
\begin{description}
 \item{Data preparation} ({\tt prepare\_predictors}) The proposed feature (call
 it $z$ is copied into local storage (from an iterator into a GSL matrix),
 centered (with its mean stored, a weighted mean if $W_0 \ne I$), and scaled by
 $\sqrt(W_0)$.
 \item{Sweep current model} Current explanatory variables are swept from the
 proposed new variable, producing ``residuals'' $z_{res}$.  This is done using
 the current weights $W_0$.
\end{description}


 \bibliography{/Users/bob/work/references/stat}
 \bibliographystyle{/Users/bob/work/papers/bst/rss}

\end{document}
